# This is a file of useful commands associated with Roundup.  Enjoy.  -TFD


Test lsfdo and filemsg on orchestra:
    
    # you need to set 'username' and 'password'
    cd /www/dev.roundup.hms.harvard.edu && TEST_LSFDO_WAIT=30 DONES_DB_URL=mysql://username:password@dev.mysql.orchestra/devroundup venv/bin/nosetests


# Quest for Orthologs

## Build a new Quest for Orthologs dataset

Deploying the code for the first time, creating a virtual environment:

    cd ~work/roundup/app
    # Do a full deployment of code from scratch, including creating a virtual environment and installing requirements.
    fab ds:qfo_2013_04 full

Deploying the code all the rest of the time:

    # Alternatively, after creating the virtualenv, just deploy the code and config.
    fab ds:qfo_2013_04 most

On an orchestra compute node, run the quest for orthologs workflow.  Currently
this will "fail" when launching jobs on orchestra to format genomes or compute
orthologs.  This is fine.  Just wait for the lsf jobs to finish and rerun the
workflow and it will pick up where it left off:

    cd ~/roundup/code/qfo_2013_04
    ROUNDUP_MYSQL_CREDS_FROM_CNF=True venv/bin/python app/quest.py workflow /groups/public+cbi/sites/roundup/datasets/qfo_2013_04



################################
# VERSION CONTROL AND DEPLOYMENT
################################

# deploy code to dev or prod
python build.py --deployenv=orch_dev
python build.py --deployenv=orch_prod


# commit changes, push to orchestra repo, and move changes from master (dev) branch to the production branch
git commit -a
git push
git checkout prod
git merge --no-ff master
git tag -a v2.0.2 -m 'performance enhancements for sources page and getting genomes for dropdown boxes.'
git push
git checkout master


###################################
# THE MAKING OF A DATASET / RELEASE
###################################

# Figure out which release this is.  If the latest release is 2, then this release is 3 and the dataset is /groups/cbi/roundup/datasets/3
# many of these steps take a while, minutes to hours.  computeJobs takes weeks (on a large dataset).

# deploy code to the dataset.  Many steps take hours, days, or weeks, so code is deployed independent from dev or prod code
# so they can vary without interfering with each other.
fab ds:3 init_deploy_env

# some commands require >8GB RAM, so increase the resource requirement on lsf so job is not killed.
bsub -Is -q shared_int_12h -R "rusage[mem=16384]" bash

# change dir to where the code is located
cd /groups/cbi/roundup/datasets/3/code/webapp


# create a dataset on orchestra
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.prepareDataset(ds)"

# download and process sources
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.downloadSources(ds)"
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.processSources(ds)"

# create a dataset on orchestra, the directories and other bits that make up a dataset.
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.prepareDataset(ds)"
# download and process sources
# sometimes this needs to be rerun a few times, since the downloads of the large dat files can fail.
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.downloadSources(ds)"
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.processSources(ds)"
# extract some metadata
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.extractUniprotRelease(ds)"
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.extractTaxonData(ds)"
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.extractFromGeneOntology(ds)"
# look at dats for the counts of various genomes, completes, reference, sprot, etc.
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.examineDats(ds)"
# look for bad surprises
cat dat_surprises.txt | cut -f1 | sort | uniq


time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.setGenomesFromCounts(ds)"
# some taxons will be missing data, either b/c they do not have an ncbi taxon id or b/c they have several (perhaps they are in transition
# from one id to another)
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.findMissingTaxonToData(ds)"
# same missing taxons as in uniprot release 2011_10
# fix the missing taxon data
time python -c "import roundup_dataset; 
ds = '/groups/cbi/roundup/datasets/3';
missingData = {
'654926': {roundup_dataset.NAME: 'Ectocarpus siliculosus virus 1 (isolate New Zealand/Kaikoura/1988)',
           roundup_dataset.CAT_CODE: 'V',
           roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
'654925': {roundup_dataset.NAME: 'Emiliania huxleyi virus 86 (isolate United Kingdom/English Channel/1999)',
           roundup_dataset.CAT_CODE: 'V',
           roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
'1054147': {roundup_dataset.NAME: 'Dictyostelium fasciculatum (strain SH3)',
           roundup_dataset.CAT_CODE: 'E',
           roundup_dataset.CAT_NAME: 'Eukaryota'},
'587202': {roundup_dataset.NAME: 'Variola virus (isolate Human/Japan/Yamada MS-2(A)/1946)',
           roundup_dataset.CAT_CODE: 'V',
           roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
'587203': {roundup_dataset.NAME: 'Variola virus (isolate Human/Brazil/v66-39/1966)',
           roundup_dataset.CAT_CODE: 'V',
           roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
'587201': {roundup_dataset.NAME: 'Variola virus (isolate Human/South Africa/102/1965)',
           roundup_dataset.CAT_CODE: 'V',
           roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
}
roundup_dataset.setMissingTaxonToData(ds, missingData)"

# confirm that you fixed all missing taxon data
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.findMissingTaxonToData(ds)"
# extract lots of information, including the fasta genomes from the dat files.
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.extractFromDats(ds)"
# format the fasta genomes.  run on lsf to speed it up.
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.formatGenomes(ds, onGrid=True)"
# prepare jobs that will compute orthologs
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.prepareJobs(ds)"

# compute the orthologs on lsf.  This takes a long time (weeks, months).  When jobs fail, rerun this command to resubmit them to lsf.
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.computeJobs(ds)"
# when all jobs are done, run again to confirm that all jobs are done.
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.computeJobs(ds)"

# compare differences between two datasets and save those differences in a "changelog".
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3';
others = ['/groups/cbi/roundup/datasets/2']
roundup_dataset.makeChangeLog(ds, others)"

# when computation is finished, extract stats and prepare files for download
bsub time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.extractDatasetStats(ds)"
bsub time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.extractPerformanceStats(ds)"
# make all genomes and orthologs available for download.
bsub time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.processGenomesForDownload(ds)"
bsub time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.collateOrthologs(ds)"
# orthoxml is skipped b/c the orthoxml files are too large.
# time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.convertToOrthoXML(ds)"
# when genomes and orthologs are ready for download, compress genomes and ortholog download files.  This takes a long time.  Could be parallelized.
bsub time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.zipDownloadPaths(ds)"


#####################################
# LOADING A DATASET INTO THE DATABASE
#####################################

# inspect the tables using phpMyAdmin to get a sense of the size of the tables you will be inserting.
# make sure there is enough space in the database
https://orchestra.med.harvard.edu/phpMyAdmin/index.php

# drop older unused dataset tables if not enough space.
# drop roundup_1_* tables, the tables from penultimate release.
# echo 'drop table if exists roundup_1_divergences, roundup_1_evalues,
# roundup_1_genomes, roundup_1_results, roundup_1_sequence, roundup_1_sequence_to_go_term' | mysql roundup
cd /groups/cbi/roundup/datasets/3/code/webapp && time python -c "import roundup_db
release = '1'
roundup_db.dropRelease(release)
roundup_db.dropReleaseResults(release)
"

# load sequences, genomes, etc.
cd /groups/cbi/roundup/datasets/3/code/webapp && \
bsub -R "rusage[mem=16384]" time python -c "import roundup_load; ds = '/groups/cbi/roundup/datasets/3'; roundup_load.loadDatabase(ds)"

# when that is done, prepare to load orthologs
cd /groups/cbi/roundup/datasets/3/code/webapp && \
bsub -R "rusage[mem=16384]" time python -c "import roundup_load; ds = '/groups/cbi/roundup/datasets/3'; roundup_load.initLoadOrthDatas(ds)"
# when that is done, load orthologs.  This can take a day or two.
cd /groups/cbi/roundup/datasets/3/code/webapp && \
bsub -R "rusage[mem=16384]" time python -c "import roundup_load; ds = '/groups/cbi/roundup/datasets/3'; roundup_load.loadOrthDatas(ds)"

# when that is done, run again to confirm that everything is loaded.  This should run quickly (a few minutes?).
cd /groups/cbi/roundup/datasets/3/code/webapp && \
bsub -R "rusage[mem=16384]" time python -c "import roundup_load; ds = '/groups/cbi/roundup/datasets/3'; roundup_load.loadOrthDatas(ds)"


#############################################
# RELEASE A DATASET TO THE PRODUCTION WEBSITE
#############################################

# Set the release date of the dataset.
# This is done on the day the dataset is released on the production website.  
# Or it can be set manually by passing in a datetime.date object.
time python -c "import roundup_dataset; ds = '/groups/cbi/roundup/datasets/3'; roundup_dataset.setReleaseDate(ds)"

# Update the current dataset of the production website
# by changing 'current_release' in the prod() task and redeploying to production
# This seems wrong.  This stuff should probably be done in the database.
cd ~/work/roundup
emacs fabfile.py
fab prod deploy


##################################################################
# RUNNING A TEST COMPUTATION USING ONLY A FEW GENOMES IN A DATASET
##################################################################

# clear any existing computation.  removes jobs, completes, and orthologs
cd /www/dev.roundup.hms.harvard.edu/webapp && python -c "import roundup_dataset;
ds = '/groups/cbi/td23/test_dataset'
roundup_dataset.cleanJobs(ds)
roundup_dataset.cleanOrthologs(ds)
"

# prepare a computation using only your test genomes
cd /www/dev.roundup.hms.harvard.edu/webapp && python -c "import roundup_dataset;
ds = '/groups/cbi/td23/test_dataset'
genomes = 'MYCGE MYCGF MYCGH MYCH1 MYCH2 MYCH7 MYCHH MYCHJ MYCHP'.split()
print genomes
roundup_dataset.setGenomes(ds, genomes) # this is the key step.  settting the genomes manually.
roundup_dataset.prepareComputation(ds, numJobs=10) # use fewer jobs than the default 4000.
"

# compute the orthologs
cd /www/dev.roundup.hms.harvard.edu/webapp && python -c "import roundup_dataset;
ds = '/groups/cbi/td23/test_dataset'
roundup_dataset.computeJobs(ds)
"


