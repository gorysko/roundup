# This is a file of useful commands associated with Roundup.  Enjoy.  -TFD


################################
# VERSION CONTROL AND DEPLOYMENT
################################

# deploy code to dev or prod
python build.py --deployenv=orch_dev
python build.py --deployenv=orch_prod


# commit changes, push to orchestra repo, and move changes from master (dev) branch to the production branch
git commit -a
git push
git checkout prod
git merge --no-ff master
git tag -a release_2.0.2 -m 'performance enhancements for sources page and getting genomes for dropdown boxes.'
git push
git checkout master


###################################
# THE MAKING OF A DATASET / RELEASE
###################################

# First figure out which release this is.  If it is the second release of 2011, then the release name 2011_02 and the dataset is /groups/cbi/roundup/datasets/2011_02
# many of these steps take a while, minutes to hours.  computeJobs takes weeks (on a large dataset).
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.prepareDataset(ds)'

# sometimes this needs to be rerun a few times, since the downloads of the large dat files can fail.
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.downloadSources(ds)'

cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.splitUniprotIntoGenomes(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.formatGenomes(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.extractFromFasta(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.extractFromIdMapping(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.extractTaxonData(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.extractFromGeneOntology(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.extractUniprotRelease(ds)'

# the taxon ids for some genomes in UniProt Release 2011_06 did not have corresponding taxon info from ncbi.  these steps are to find those taxons and manually add the data.
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.findMissingTaxonToData(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.setMissingTaxonToData(ds)'

cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.prepareComputation(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.computeJobs(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.extractDatasetStats(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.extractPerformanceStats(ds)'

# making all genomes and orthologs available for download.
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.processGenomesForDownload(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.collateOrthologs(ds)'
# orthoxml is skipped b/c the orthoxml files are too large.
# cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.convertToOrthoXML(ds)'
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.zipDownloadPaths(ds)'

# this is done on the day one releases to production.  Or it can be set manually by passing in a datetime.date object.
cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c 'import roundup_dataset; ds = "/groups/cbi/roundup/datasets/2011_02"; roundup_dataset.setReleaseDate(ds)'

#####################################
# LOADING A DATASET INTO THE DATABASE
#####################################

# load genomes, divergences, evalues, sequence, and sequence_to_go_term tables into database using dev code and prod database
cd /www/dev.roundup.hms.harvard.edu/webapp && ROUNDUP_MYSQL_SERVER=mysql.cl.med.harvard.edu ROUNDUP_MYSQL_DB=roundup time python -c "import roundup_load;
ds = '/groups/cbi/roundup/datasets/201106_dataset'
roundup_load.loadDatabase(ds)
"

# drop/create the results table.  track what has been loaded with a dones table.  use dev code and prod database.
cd /www/dev.roundup.hms.harvard.edu/webapp && ROUNDUP_MYSQL_SERVER=mysql.cl.med.harvard.edu ROUNDUP_MYSQL_DB=roundup time python -c "import roundup_load;
ds = '/groups/cbi/roundup/datasets/201106_dataset'
roundup_load.initLoadOrthDatas(ds)
"

# load orthologs into the results table.  this takes a while.  hours or days for a big dataset. 
# if this job fails, rerunning the command will resume it (more or less) from where it left off.
cd /www/dev.roundup.hms.harvard.edu/webapp && ROUNDUP_MYSQL_SERVER=mysql.cl.med.harvard.edu ROUNDUP_MYSQL_DB=roundup bsub -q shared_12h time python -c "import roundup_load;
ds = '/groups/cbi/roundup/datasets/201106_dataset'
roundup_load.loadOrthDatas(ds)
"

# after (a day or so when) the loadResulta job finishes.  Run it again to confirm that everything has loaded.
cd /www/dev.roundup.hms.harvard.edu/webapp && ROUNDUP_MYSQL_SERVER=mysql.cl.med.harvard.edu ROUNDUP_MYSQL_DB=roundup bsub -q shared_12h time python -c "import roundup_load;
ds = '/groups/cbi/roundup/datasets/201106_dataset'
roundup_load.loadOrthDatas(ds)
"

# Example of how to use ROUNDUP_MYSQL_SERVER and ROUNDUP_MYSQL_DB environment variables to make the dev code load into the prod database.
# basically prepend the env vars to the command.
cd /www/dev.roundup.hms.harvard.edu/webapp && ROUNDUP_MYSQL_SERVER=mysql.cl.med.harvard.edu ROUNDUP_MYSQL_DB=roundup time python -c "import roundup_load;
ds = '/groups/cbi/roundup/datasets/2010_01'
roundup_load.loadDatabase(ds)
"


##################################################################
# RUNNING A TEST COMPUTATION USING ONLY A FEW GENOMES IN A DATASET
##################################################################

# clear any existing computation.  removes jobs, completes, and orthologs
cd /www/dev.roundup.hms.harvard.edu/webapp && python -c "import roundup_dataset;
ds = '/groups/cbi/td23/test_dataset'
roundup_dataset.cleanJobs(ds)
roundup_dataset.cleanOrthologs(ds)
"

# prepare a computation using only your test genomes
cd /www/dev.roundup.hms.harvard.edu/webapp && python -c "import roundup_dataset;
ds = '/groups/cbi/td23/test_dataset'
genomes = 'MYCGE MYCGF MYCGH MYCH1 MYCH2 MYCH7 MYCHH MYCHJ MYCHP'.split()
print genomes
roundup_dataset.setGenomes(ds, genomes) # this is the key step.  settting the genomes manually.
roundup_dataset.prepareComputation(ds, numJobs=10) # use fewer jobs than the default 4000.
"

# compute the orthologs
cd /www/dev.roundup.hms.harvard.edu/webapp && python -c "import roundup_dataset;
ds = '/groups/cbi/td23/test_dataset'
roundup_dataset.computeJobs(ds)
"


