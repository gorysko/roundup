#!/usr/bin/env python

'''
## CONCEPTS TO LEARN
* Dataset: ds, the id of a dataset.  also happens to be a directory path, but that is an implementation detail. 
* Sources: a dir containing all the files needed to create the genomes and the metadata about those genomes: gene names, go terms, gene descriptions, genome names, etc.
* Genomes: A dir containing dirs named after each genome and containing a fasta file and blast indexes named after the genome.
  a genome id is an uniprot species identifier.  http://ca.expasy.org/sprot/userman.html#ID_line
* Jobs: a dir containing one dir for each job.  a job is the computational unit used for parallelism.  a job is assigned pairs to run rsd on.
  it runs rsd and stores the orthologs in the orthologs dir in a file named for the job.
* Orthologs: a dir containing files of orthologs generated by rsd.  the file lines describe orthologs by the query genome, subject genome, div, evalue, qid, sid, and distance
* Pair: a tuple of a pair of genomes, the "query" genome and "subject" genome.  a well-formed pair is sorted.
* Metadata: caches information about a dataset for quick retrieval.  
* Completes: track which stages of creating a dataset or running a computation have been completed.  There are two completes, one in the database that is concurrency-safe
  and scalable to millions of entries (useful for tracking complete pairs), and one on the filesystem, which is human editable and stored with the dataset so it is not
  sensitive to which code (dev or prod) is run.
* Database: 

'''

# A description of Uniprot Knowledgebase files:
# http://ca.expasy.org/sprot/userman.html

# A list of all species ids abbreviations, their kingdom, taxon id, and official names, common names, and synonyms.  Cool!
# http://www.expasy.org/cgi-bin/speclist


'''
# Fiddling with completes and dataset state to get completed stuff to run again.
# remove existing orthologs
rm -rf /groups/cbi/td23/roundup_uniprot/test_dataset/orthologs/*
# remove existing jobs
rm -rf /groups/cbi/td23/roundup_uniprot/test_dataset/jobs/*
# drop completes table so jobs will run
echo 'drop table key_value_store_roundup_ds_test_dataset' | mysql devroundup
# remove prepare computation complete too
emacs /groups/cbi/td23/roundup_uniprot/test_dataset/steps.complete.txt
'''

# standard library modules
import collections
import glob
import gzip
import json
import datetime
import itertools
import logging
import os
import random
import shutil
import subprocess
import sys
import time
import urlparse
import uuid

# 3rd party modules
import Bio.SeqIO

# our modules
import config
import dbutil
import fasta
import LSF
import nested
import kvstore
import roundup_common
import util
import roundup_db
import lsfdispatch
import rsd


MIN_GENOME_SIZE = 200 # ignore genomes with fewer sequences
DIR_MODE = 0775 # directories in this dataset are world readable and group writable.

# short keys, to save RAM space, used for the geneToData dicts.  also used for termToData.
NAME = 'n'
DESC = 'd'
GENOME = 'g'
GO_TERMS = 't'
GENE_IDS = 'i'
TYPE = 'y'

# METADATA STORES, keys for getData()
DATASET = 'dataset'
GENES = 'genes' 
GENE_TO_GENOME = 'gene_to_genome'
GENE_TO_NAME = 'gene_to_name'
GENE_TO_DESC = 'gene_to_desc'
GENE_TO_GO_TERMS = 'gene_to_go_terms'
GENE_TO_GENE_IDS = 'gene_to_gene_ids'
TERM_TO_DATA = 'term_to_data'
GENOME_TO_GENES = 'genome_to_genes'
NEW_PAIRS = 'new_pairs'
OLD_PAIRS = 'old_pairs'


def main(ds):

    steps = [(prepareDataset, 'prepare dataset'),
             (downloadCurrentUniprot, 'download current uniprot'),
             (unzipUniprotSources, 'unzip uniprot sources'),
             (splitUniprotIntoGenomes, 'split sources into fasta genomes'),
             (formatGenomes, 'format genomes'),
             (extractGeneIdsAndGoTerms, 'extract'),
             (extractGenomeAndGeneNames, 'extract'),
             (prepareComputation, 'prepare computation'),
             ]

    for func, tag in steps:
        if not isStepComplete(ds, tag):
            func(ds)
            markStepComplete(ds, tag)
            

        

#######################
# MAIN DATASET PIPELINE
#######################

def prepareDataset(ds):
    '''
    Make the directory structure, tables, etc., a new dataset needs.
    '''
    resetCompletes(ds)
    resetStats(ds)
    for path in (ds, getGenomesDir(ds), getOrthologsDir(ds), getJobsDir(ds), getSourcesDir(ds), getLoadDir(ds)):
        if not os.path.exists(path):
            os.makedirs(path, DIR_MODE)
    setMetadata(ds, {}) # initialize a blank metadata for the dataset

def downloadCurrentUniprot(ds):
    '''
    Download uniprot files containing protein fasta sequences and associated meta data (gene names, go annotations, dbxrefs, etc.)
    Download all these files to have a comprehensive set of data for a uniprotkb release:

    Index of /pub/databases/uniprot/current_release/knowledgebase/complete/
    NameSizeDate Modified
    README4.4 kB4/5/11 2:00:00 PM
    README.gunzip335 B4/5/11 2:00:00 PM
    README.reldate694 B4/5/11 2:00:00 PM
    README.varsplic10.5 kB4/5/11 2:00:00 PM
    4/5/11 2:00:00 PM
    reldate.txt151 B4/5/11 2:00:00 PM
    uniprot.xsd47.1 kB4/5/11 2:00:00 PM
    uniprot_sprot.dat.gz418 MB4/5/11 2:00:00 PM
    uniprot_sprot.fasta.gz73.8 MB4/5/11 2:00:00 PM
    uniprot_sprot.xml.gz722 MB4/5/11 2:00:00 PM
    uniprot_sprot_varsplic.fasta.gz6.2 MB4/5/11 2:00:00 PM
    uniprot_trembl.dat.gz5.5 GB4/5/11 2:00:00 PM
    uniprot_trembl.fasta.gz2.6 GB4/5/11 2:00:00 PM
    uniprot_trembl.xml.gz10.7 GB4/5/11 2:00:00 PMdocs/[parent directory]
    
    Index of /pub/databases/uniprot/current_release/knowledgebase/idmapping/
    NameSizeDate Modified
    README2906 B4/5/11 2:00:00 PM
    idmapping.dat.example254 kB4/5/11 2:00:00 PM
    idmapping.dat.gz914 MB4/5/11 2:00:00 PM
    idmapping_selected.tab.example948 kB4/5/11 2:00:00 PM
    idmapping_selected.tab.gz549 MB4/5/11 2:00:00 PM[parent directory]
    '''
    
    print 'downloadCurrentUniprot: {}'.format(ds)

    # get as much as possible about the current uniprotkb release, in case it is needed later.
    currentUrl = 'ftp://ftp.uniprot.org/pub/databases/uniprot/current_release'
    files = ['relnotes.txt',
             'knowledgebase/complete/README', 
             'knowledgebase/complete/README.gunzip', 
             'knowledgebase/complete/README.reldate', 
             'knowledgebase/complete/README.varsplic', 
             'knowledgebase/complete/reldate.txt', 
             'knowledgebase/complete/uniprot.xsd', 
             'knowledgebase/complete/uniprot_sprot.dat.gz', 
             'knowledgebase/complete/uniprot_sprot.fasta.gz', 
             'knowledgebase/complete/uniprot_sprot.xml.gz', 
             'knowledgebase/complete/uniprot_sprot_varsplic.fasta.gz', 
             'knowledgebase/complete/uniprot_trembl.dat.gz', 
             'knowledgebase/complete/uniprot_trembl.fasta.gz', 
# skip enormous trembl xml file b/c curl might have trouble downloading it. 'knowledgebase/complete/uniprot_trembl.xml.gz',
             'knowledgebase/idmapping/README', 
             'knowledgebase/idmapping/idmapping.dat.example', 
             'knowledgebase/idmapping/idmapping.dat.gz', 
             'knowledgebase/idmapping/idmapping_selected.tab.example', 
             'knowledgebase/idmapping/idmapping_selected.tab.gz', 
             ]
    
    sourcesDir = getSourcesDir(ds)
    for f in files:
        url = currentUrl + '/' + f
        dest = os.path.join(sourcesDir, f)
        if not os.path.exists(os.path.dirname(dest)):
            os.makedirs(os.path.dirname(dest), DIR_MODE)
        print 'downloading {} to {}...'.format(url, dest)
        if isStepComplete(ds, 'download', url):
            print '...skipping because already downloaded.'
            continue
        cmd = 'curl --remote-time --output '+dest+' '+url
        subprocess.check_call(cmd, shell=True)
        print
        markStepComplete(ds, 'download', url)
        print '...done.'
        pause = 20
        print 'pausing for {} seconds.'.format(pause)
        time.sleep(pause)
    updateMetadata(ds, {'sources': {'download_time': str(datetime.datetime.now()), 'currentUrl': currentUrl, 'files': files}})
    print 'done downloading sources.'


def unzipUniprotSources(ds):
    '''
    unzipping the files makes them take up more space, but it makes processing them faster.  especially if you have to process them
    several times.
    '''
    files = [
             'knowledgebase/complete/uniprot_sprot.dat.gz', 
             'knowledgebase/complete/uniprot_sprot.fasta.gz', 
             'knowledgebase/complete/uniprot_trembl.dat.gz', 
             'knowledgebase/complete/uniprot_trembl.fasta.gz', 
             'knowledgebase/idmapping/idmapping_selected.tab.gz', 
             ]
    for f in files:
        path = os.path.join(getSourcesDir(ds), f)
        if os.path.exists(path):
            print 'unzipping {}...'.format(f)
            subprocess.check_call(['gunzip', path])


def splitUniprotIntoGenomes(ds, writing=True, skipDirs=False, cleanDirs=False, bufSize=5000000, joinBeforeWrite=True):
    '''
    create separate fasta files for each complete genome in the uniprot (sprot and trembl) data.
    do not create files for genomes that are too small.
    also save maps from genome to ncbi taxon id, and genome to size (# of sequences).
    iterates once through the dat files to find out which sequences/entries belong to complete proteomes (and to get taxons and sequence counts.)
    then iterates once through fasta files to write the fasta sequences for each genome.  
    '''
    seqToGenome = {} # track which sequences belong to which genome.  store sequences of all complete genomes
    genomeToTaxon = {} # maps each genome to an ncbi taxon id
    genomeToCount = collections.defaultdict(int) # maps each genome to the number of sequences (in the dat files) for that genome.  should == num seqs in fasta files.
    
    # the dats have all the info needed to make fasta files, but the namelines in the uniprot fasta files are nice and hard to reconstruct from the dats.
    dats = [os.path.join(getSourcesDir(ds), 'knowledgebase/complete/uniprot_sprot.dat'), os.path.join(getSourcesDir(ds), 'knowledgebase/complete/uniprot_trembl.dat')]
    fastas = [os.path.join(getSourcesDir(ds), 'knowledgebase/complete/uniprot_sprot.fasta'), os.path.join(getSourcesDir(ds), 'knowledgebase/complete/uniprot_trembl.fasta')]
    # dats = [os.path.join(getSourcesDir(ds), 'knowledgebase/complete/uniprot_sprot.dat')]
    # fastas = [os.path.join(getSourcesDir(ds), 'knowledgebase/complete/uniprot_sprot.fasta')]
    # dats = ['/groups/cbi/td23/roundup_uniprot/uniprot_sprot_test.dat']
    # fastas = ['/groups/cbi/td23/roundup_uniprot/uniprot_sprot_test.fasta']

    # gather which sequences belong to complete genomes, by parsing uniprot dat files.
    # also get the ncbi taxon ids and a count of the sequences per genome.
    print datetime.datetime.now()
    for path in dats:
        print 'gathering ids in {}...'.format(path), datetime.datetime.now()
        with open(path) as fh:
            i = 0
            seqId = genome = taxon = None
            complete = False
            for line in fh:
                code = line[:2]
                if code == 'ID':
                    if i % 100000 == 0: print i
                    i += 1
                    genome = line.split()[1].split('_')[1]
                elif code == 'AC' and not seqId: # can be one or more AC lines
                    # e.g. AC   Q16653; O00713; O00714; O00715; Q13054; Q13055; Q14855; Q92891;
                    seqId = line.split()[1].split(';')[0]
                elif code == 'OX':
                    # e.g. OX   NCBI_TaxID=9606;
                    taxon = line.split()[1].split('=')[1].split(';')[0]
                elif code == 'KW' and not complete and line.find('Complete proteome') != -1: # can be zero or more KW lines
                    complete = True
                elif code == '//': # end of sequence
                    if complete:
                        seqToGenome[seqId] = genome
                        genomeToCount[genome] += 1
                        if genome not in genomeToTaxon:
                            genomeToTaxon[genome] = taxon
                        elif genomeToTaxon[genome] != taxon:
                            with open(os.path.join(ds, 'errors.process_uniprot.txt'), 'a') as fh:
                                fh.write('{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\n'.format(datetime.datetime.now().isoformat(), 'genome_has_two_taxons', seqId,
                                                                                      genome, genomeToTaxon[genome], taxon, os.path.basename(path)))
                    seqId = genome = taxon = None
                    complete = False

    # filter out genomes that are too small, and their respective sequences.
    genomes = [genome for genome, count in genomeToCount.items() if count >= MIN_GENOME_SIZE]
    seqToGenome = dict((seqId, genome) for seqId, genome in seqToGenome.items() if genome in genomes)
            
    # print a sorted list of genomes and number of sequences
    for genome, count in sorted([(genome, genomeToCount[genome]) for genome in genomes], key=lambda x: (x[1], x[0])):
        print genome, count
        pass

    # helper function to create genome directories for genomes not yet seen
    dirGenomes = set()
    def makeGenomeDir(genome):
        if not skipDirs and genome not in dirGenomes:
            dirGenomes.add(genome)
            genomePath = getGenomePath(ds, genome)
            if not os.path.exists(genomePath):
                os.mkdir(genomePath, DIR_MODE)

    # helper function to write to a genome file or a single file.  for performance testing.
    oneFileFH = []
    writeGenomes = set()
    def writeToGenome(genome, data):
        if not writing:
            return
        if genome not in writeGenomes: # first time writing to the genome
            mode = 'w'
            writeGenomes.add(genome)
        else:
            mode = 'a'
        with open(getGenomeFastaPath(ds, genome), mode) as outfh:
            if joinBeforeWrite:
                output = ''.join(data)
                outfh.write(output)
            else:
                for line in data:
                    outfh.write(line)

    # remove any pre-existing genomes.
    if cleanDirs:
        print 'cleaning genomes directory...', datetime.datetime.now()
        cleanGenomes(ds)

    # make a directory for each genome in which to put the fasta file.
    print datetime.datetime.now()
    print 'initializing {} genome directories...'.format(len(genomes))
    for genome in genomes:
        makeGenomeDir(genome)

    # write the fasta sequences to the fasta files.  lots of caching here b/c filessystem is slow to open and close files.
    print datetime.datetime.now()
    for path in fastas:
        # iterate through every line in the fasta file.
        # when we find a nameline, check if the id is from a complete genome.
        # if it is, check if the genome dir has been created yet and if not create it.
        # if the genome is different from the last genome, switch files we are writing to.
        # write every line from a complete genome to the right file.
        print 'splitting {} into genomes'.format(path)
        genomeToLines = collections.defaultdict(list)
        for i, lines in enumerate(fasta.readFastaLines(path)):
            seqId = fasta.idFromName(lines[0]) # uniprot accession
            if seqId in seqToGenome: # write all lines from complete genomes to the fasta file
                genomeToLines[seqToGenome[seqId]].extend(lines)
            if i % bufSize == 0:
                # write out collected lines to the various genome fasta files
                print 'writing collected lines to fasta files for {} genomes...'.format(len(genomeToLines)), datetime.datetime.now()
                for genome in genomeToLines:
                    writeToGenome(genome, genomeToLines[genome])
                genomeToLines.clear()
                print 'collecting more lines...', datetime.datetime.now()
        # done with path.
        # write out collected lines to the various genome fasta files
        print 'finishing writing collected lines to fasta files for {} genomes...'.format(len(genomeToLines)), datetime.datetime.now()
        for genome in genomeToLines:
            writeToGenome(genome, genomeToLines[genome])
        print 'done writing collected lines.', datetime.datetime.now()
                
    print 'getting genomes and updating metadata...', datetime.datetime.now()
    
    setGenomes(ds)
    updateMetadata(ds, {'genomeToTaxon': genomeToTaxon, 'genomeToCount': genomeToCount})
    print 'all done.', datetime.datetime.now()
    

def formatGenomes(ds):
    '''
    format genomes for blast.
    '''
    print 'formatting genomes. {}'.format(ds)
    genomes = getGenomes(ds)
    for genome in genomes:
        fastaPath = getGenomeFastaPath(ds, genome)
        print 'format {}'.format(genome)
        rsd.formatForBlast(fastaPath)
    print 'done formatting genomes'


def parseUniprotNameline(nameline):
    '''
    Parse uniprot nameline to get the uniprot sequence accession, gene description, gene name, organism name and
    organism id (e.g. HUMAN or MYCGE)
    A uniprot nameline must have: acc, orgId
    optional: geneDesc, geneName, orgName
    example nameline: >sp|P38398|BRCA1_HUMAN Breast cancer type 1 susceptibility protein OS=Homo sapiens GN=BRCA1 PE=1 SV=2
    example results: acc: P38398, geneName (GN): BRCA1, orgId: HUMAN, orgName (OS): Homo sapiens, geneDesc: Breast cancer type 1 susceptibility protein
    returns: a dict containing keys for acc, geneDesc, geneName, orgName, and orgId.  Some values 
    '''
    # split apart nameline into acc, orgName, and geneName.
    # example: >sp|P38398|BRCA1_HUMAN Breast cancer type 1 susceptibility protein OS=Homo sapiens GN=BRCA1 PE=1 SV=2
    # sometimes no gene name: >sp|P38398|BRCA1_HUMAN Breast cancer type 1 susceptibility protein OS=Homo sapiens PE=1 SV=2
    # '>sp|P38398|BRCA1_HUMAN', 'Breast cancer type 1 susceptibility protein OS=Homo sapiens GN=BRCA1 PE=1 SV=2'
    accAndOrgId, descOrgGNEtc = nameline.strip().split(None, 1) # None splits on whitespace.  Only do the first split.
    # '>sp', 'P38398', 'BRCA1_HUMAN'
    etc, acc, almostOrgId = accAndOrgId.split('|')
    # 'BRCA1', 'HUMAN'
    etc, orgId = almostOrgId.rsplit('_', 1) # org abbr example: HUMAN.  i.e uniprot species id
    # 'Breast cancer type 1 susceptibility protein', 'Homo sapiens GN=BRCA1 PE=1 SV=2'
    geneDesc, orgGNEtc = (s.strip() for s in descOrgGNEtc.split('OS=', 1))
    # 'Homo sapiens GN=BRCA1 ', '1 SV=2'
    orgGN, etc = orgGNEtc.split('PE=', 1)
    if orgGN.find('GN=') == -1:
        orgName, geneName = (s.strip() for s in (orgGN, ''))
    else:
        # 'Homo sapiens', 'BRCA1'
        orgName, geneName = (s.strip() for s in orgGN.split('GN=', 1))

    return {'acc': acc, 'geneDesc': geneDesc, 'geneName': geneName, 'orgName': orgName, 'orgId': orgId}


def extractFromFasta(ds):
    '''
    parse from the namelines the uniprot fasta files the gene name, genome/organism name
    The organism name The gene name is optional.
    The organism name must be the same in every nameline (for the same genome).
    There must exactly one nameline per sequence.
    example nameline: >sp|P38398|BRCA1_HUMAN Breast cancer type 1 susceptibility protein OS=Homo sapiens GN=BRCA1 PE=1 SV=2
    gene name (GN): BRCA1, organism abbr: HUMAN, organism (OS): Homo sapiens
    '''
    # geneToData = {} # map each gene to data about the gene: name, ncbi gene ids, go terms, description, genome.
    genes = []
    geneToName = {}
    geneToDesc = {}
    geneToGenome = {}
    genomeToGenes = {}
    genomeToName = {}
    
    # EXCEPTIONS to the exactly one name for a genome rule, found in UniProtKB release 2011_04.
    # taxonid | 2nd name found | 1st name found
    # 246196 | Mycobacterium smegmatis (strain ATCC 700084 / mc(2)155) | Mycobacterium smegmatis
    # 321332 | Synechococcus sp. (strain JA-2-3B'a(2-13)) | Synechococcus sp.
    # 771870 | Sordaria macrospora (strain ATCC MYA-333 / DSM 997 / K(L3346) / K-hell) | Sordaria macrospora
    # 290318 | Prosthecochloris vibrioformis (strain DSM 265) | Prosthecochloris vibrioformis (strain DSM 265) (strain DSM 265)
    # 710128 | Mycoplasma gallisepticum (strain R(high / passage 156)) | Mycoplasma gallisepticum
    # 375286 | Janthinobacterium sp. (strain Marseille) (Minibacterium massiliensis) | Janthinobacterium sp. (strain Marseille)
    # since the sprot/trembl data has genomes that have different names in sprot and trembl (see exceptions listed above),
    # ignore these bad genomes.
    # badGenomes = set(['246196', '321332', '771870', '290318', '710128', '375286'])
    # FIXED in 2011_06 release
    badGenomes = set()
    
    for i, genome in enumerate(getGenomes(ds)):
        if i % 20 == 0: print i
        path = getGenomeFastaPath(ds, genome)
        if genome not in genomeToGenes:
            genomeToGenes[genome] = []
        # print '{}: extracting from {}'.format(i, path)
        for nameline, seq in fasta.readFasta(path):
            try:
                data = parseUniprotNameline(nameline)
            except:
                print (i, genome, path, nameline)
                raise
            gene, geneDesc, geneName, genomeName  = data['acc'], data['geneDesc'], data['geneName'], data['orgName']

            # a sequence must be encountered only once.
            # if geneToData.has_key(gene):
            if geneToName.has_key(gene):
                raise Exception('Sequence encountered more than one time!', gene, geneName, geneDesc, nameline, genome, path, i)
            else:
                # gene name and description are optional
                genes.append(gene)
                # geneToData[gene] = {NAME: geneName, DESC: geneDesc, GENOME: genome}
                geneToName[gene] = geneName
                geneToDesc[gene] = geneDesc
                geneToGenome[gene] = genome
                genomeToGenes[genome].append(gene)
                
            # assert exactly one name for genome.  b/c some genomes have a different name in sprot and trembl, make case-by-case exceptions.
            if not genomeToName.has_key(genome):
                genomeToName[genome] = genomeName
            elif genomeToName[genome] != genomeName and genome not in badGenomes:
                with open(os.path.join(ds, 'errors.extract_uniprot.txt'), 'a') as fh:
                    msg = '{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\n'
                    msg = msg.format(datetime.datetime.now().isoformat(), 'genome_has_two_names', gene, genome, genomeToName[genome], genomeName, os.path.basename(path))
                    fh.write(msg)
                    # print msg

    setData(ds, GENES, genes)
    setData(ds, GENE_TO_NAME, geneToName)
    setData(ds, GENE_TO_DESC, geneToDesc)
    setData(ds, GENE_TO_GENOME, geneToGenome)
    setData(ds, GENOME_TO_GENES, genomeToGenes)
    updateMetadata(ds, {'genomeToName': genomeToName})

    # setGenes(ds, genes)
    # setGeneToData(ds, geneToData)
    # setGeneToName(ds, geneToName)
    # setGeneToDesc(ds, geneToDesc)
    # setGeneToGenome(ds, geneToGenome)
    # with open(os.path.join(ds, 'genome_to_name.json'), 'w') as fh:
    #     json.dump(genomeToName, fh, indent=2)
        

def extractFromIdMapping(ds):
    '''
    From ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/README:
    2) idmapping_selected.tab
    We also provide this tab-delimited table which includes
    the following mappings delimited by tab:

        1. UniProtKB-AC
        2. UniProtKB-ID
        3. GeneID (EntrezGene)
        4. RefSeq
        5. GI
        6. PDB
        7. GO
        8. IPI
        9. UniRef100
        10. UniRef90
        11. UniRef50
        12. UniParc
        13. PIR
        14. NCBI-taxon
        15. MIM
        16. UniGene
        17. PubMed
        18. EMBL
        19. EMBL-CDS
        20. Ensembl
        21. Ensembl_TRS
        22. Ensembl_PRO
        23. Additional PubMed
    Writes two dicts to files, one mapping gene to go terms, the other mapping gene to ncbi gene id.
    '''

    geneSet = set(getData(ds, GENES)) # a set to improve lookup performance.  all the seq ids from the genome fasta files.
    # geneToData = getGeneToData(ds)
    geneToGoTerms = {}
    geneToGeneIds = {}
    with open(os.path.join(getSourcesDir(ds), 'knowledgebase/idmapping/idmapping_selected.tab')) as fh:
        for i, line in enumerate(fh):
            if i % 500000 == 0: print i
            seqId, b, geneIdsStr, d, e, f, goTermsStr, etc = line.split('\t', 7)
            if seqId in geneSet: # ignore sequences not in our set of genes (i.e. from genomes we ignore)
                goTerms = goTermsStr.split('; ') if goTermsStr else []
                geneIds = geneIdsStr.split('; ') if geneIdsStr else []
                # print line
                # print (seqId, b, geneIds, d, e, f, goTerms)
                # geneToData[seqId][GO_TERMS] = goTerms
                # geneToData[seqId][GENE_IDS] = geneIds
                geneToGoTerms[seqId] = goTerms
                geneToGeneIds[seqId] = geneIds

    setData(ds, GENE_TO_GO_TERMS, geneToGoTerms)
    setData(ds, GENE_TO_GENE_IDS, geneToGeneIds)
    # setGeneToData(ds, geneToData)
    # setGeneToGoTerms(ds, geneToGoTerms)
    # setGeneToGeneIds(ds, geneToGeneIds)


def extractGoTermData(ds):
    '''
    note: this works, b/c the go database on dev.mysql and mysql is the same.
    creates a lookup for go term accessions to name and term_type.  e.g. 'GO:2000779' => 'regulation of double-strand break repair', 'biological_process'
    '''

    termToData = {}
    sql = "select acc, name, term_type from go.term where is_obsolete = 0 and term_type = 'biological_process'"
    with config.dbConnCM() as conn:
        results = dbutil.selectSQL(sql=sql, conn=conn)
        for row in results:
            termToData[row[0]] = {NAME: row[1], TYPE: row[2]}
    setData(ds, TERM_TO_DATA, termToData)

            

def prepareComputation(ds, oldDs=None, numJobs=4000, pairs=None):
    '''
    ds: dataset to ready for computation
    pairs: if not None, this list of genome pairs specifies which pairs will be computed.  useful for creating small test computations.
    oldDs: if not None, only compute pairs that have an updated genome in the new dataset.  for other pairs with no updated genomes, transfer results from oldDs to ds.
    numJobs: split computation into this many jobs.  More jobs = shorter jobs, better parallelism.
      Fewer jobs = fewer dirs and files to make isilon run slowly and overload lsf queue.
      Recommendation: <= 10000.

    '''
    print 'prepare computation for {}'.format(ds)

    if pairs:
        newPairs = pairs
        oldPairs = []
    elif oldDs:
        # get new and old pairs
        newPairs, oldPairs = getNewAndDonePairs(ds, oldDs)
        # get orthologs for old pairs and dump them into a orthologs file.
    else:
        newPairs = getPairs(ds)
        oldPairs = []
        
    # save the pairs to be computed and the pairs whose orthologs need to be moved.
    print 'saving new and old pairs'
    # setNewPairs(ds, newPairs)
    # setOldPairs(ds, oldPairs)
    setData(ds, NEW_PAIRS, newPairs)
    setData(ds, OLD_PAIRS, oldPairs)
    print 'new pair count:', len(newPairs)
    print 'old pair count:', len(oldPairs)

    # create up to N jobs for the pairs to be computed.
    # each job contain len(pairs)/N pairs, except if N does not divide len(pairs) evenly, some jobs get an extra pair.
    # Distribute pairs among jobs so that each job will have about the same running time.
    # Ideally job running time would be explictly balanced, but currently pairs are just assigned randomly.
    # e.g. if you had 11 pairs (1,2,3,4,5,6,7,8,9,10,11) and 3 jobs, the pairs would be split into these jobs: (1,2,3,4),(5,6,7,8),(9,10,11)
    random.shuffle(newPairs)
    for i, jobPairs in enumerate(util.splitIntoN(newPairs, numJobs)):
        if i % 100 == 0: print 'preparing job', i
        job = 'job_{}'.format(i)
        print 'job:', job
        print 'len(jobPairs)=', len(jobPairs)
        jobDir = getJobDir(ds, job)
        print 'jobDir:', jobDir
        os.makedirs(getJobDir(ds, job), 0770)
        setJobPairs(ds, job, jobPairs)

    getJobs(ds, refresh=True) # refresh the cached metadata
    

def getNewAndDonePairs(ds, oldDs):
    '''
    ds: current dataset, containing genomes
    oldDs: a previous dataset.
    Sort all pairs of genomes in the current dataset into todo and done pairs:
      new pairs need to be computed because each pair contains at least one genome that does not exist in or is different from the genomes of the old dataset.
      done pairs do not need to be computed because the genomes of each pair are the same as the old dataset, so the old orthologs are still valid.
    returns: the tuple, (newPairs, donePairs)
    '''
    raise Exception('untested')
    genomesAndPaths = getGenomesAndPaths(ds)
    oldGenomesAndPaths = getGenomesAndPaths(oldDs)
    # a new genome is one not in the old genomes or different from the old genome.
    newGenomes = set()
    for genome in genomesAndPaths:
        if genome not in oldGenomesAndPaths or not roundup_common.genomePathsEqual(genomesAndPaths[genome], oldGenomesAndPaths[genome]):
            newGenomes.add(genome)

    pairs = roundup_common.getPairs(genomesAndPaths.keys())
    newPairs = []
    oldPairs = []
    for pair in pairs:
        if pair[0] in newGenomes or pair[1] in newGenomes:
            newPairs.append(pair)
        else:
            oldPairs.append(pair)

    return (newPairs, oldPairs)


def moveOldOrthologs(ds, oldDs, pairs):
    '''
    pairs: the pairs that do not need to be computed because their genomes have not changed.
    Move orthologs files from the old dataset to the new dataset.
    '''
    raise Exception('unimplemented')


#########
# TESTING
#########

def splitOrthologsIntoOldResultsFiles(ds, here='.'):
    '''
    here: directory in which to save results files.
    This is useful for comparing new results, where orthologs for different pairs and parameter combinations are combined in a single file,
    against old results, where every pair x param combo has its own file for orthologs.
    '''
    for job in getJobs(ds):
        for (qdb, sdb, div, evalue, qhit, shit, dist) in getJobOrthologs(ds, job):
            with open(os.path.join(here, '{}.aa_{}.aa_{}_{}'.format(qdb, sdb, div, evalue)), 'a') as fh:
                fh.write('{} {} {}\n'.format(shit, qhit, dist))

        
###############
# DATASET STUFF
###############

def getDatasetId(ds):
    return os.path.basename(ds)


def getGenomesDir(ds):
    return os.path.join(ds, 'genomes')


def getJobsDir(ds):
    return os.path.join(ds, 'jobs')

    
def getOrthologsDir(ds):
    return os.path.join(ds, 'orthologs')

    
def getSourcesDir(ds):
    return os.path.join(ds, 'sources')


def getLoadDir(ds):
    '''
    used to load the database
    '''
    return os.path.join(ds, 'load')


###################
# CLEANING FUNCTION
###################



def cleanOrthologs(ds):
    '''
    remove everything in the orthologs dir.  useful for resetting a computation
    '''
    print 'removing orthologs'
    od = getOrthologsDir(ds)
    for path in [os.path.join(od, f) for f in os.listdir(od)]:
        print 'removing {}'.format(path)
        os.remove(path)


def cleanJobs(ds):
    '''
    removes all the jobs in the dataset.  can be useful for clearing out a test computation
    '''
    # remove all job dirs
    print 'removing jobs'
    jobs = getJobs(ds, refresh=True)
    for job in jobs:
        path = getJobDir(ds, job)
        if os.path.exists(path):
            print 'removing {}'.format(path)
            shutil.rmtree(path)
    # reset jobs cache
    getJobs(ds, refresh=True)
    # drop the table that stores job completes
    print 'resetting completes...'
    resetCompletes(ds)
    print 'resetting stats...'
    resetStats(ds)


def cleanGenomes(ds):
    '''
    remove all the genomes in the genomes dir
    '''
    genomes = getGenomes(ds)
    for genome in genomes:
        path = getGenomePath(ds, genome)
        if os.path.exists(path):
            print 'removing {}'.format(path)
            # shutil.rmtree(path)
    # reset genomes cache
    setGenomes(ds)


#################
# RUN COMPUTATION
#################

def computeJobs(ds):
    '''
    submit all incomplete and non-running jobs to lsf, so they can compute their respective pairs.
    '''
    jobs = getJobs(ds)
    util.writeToFile('computeJobs: ds={}\njobs={}\n'.format(ds, jobs), os.path.join(ds, makeUniqueDatetimeName(prefix='roundup_compute_history_')))
    for job in jobs:
        if isComplete(ds, 'job', job):
            print 'job is already complete:', job
            continue
        if isJobRunning(ds, job):
            print 'job is already running:', job
            continue
        funcName = 'roundup_dataset.computeJob'
        keywords = {'ds': ds, 'job': job}
        # reserve 500MB in /tmp for duration of job to avoid nodes where someone, not naming any names, has used too much space.
        lsfOptions = ['-R "rusage[tmp=500]"', '-q '+LSF.LSF_LONG_QUEUE, roundup_common.ROUNDUP_LSF_OUTPUT_OPTION, '-J '+getComputeJobName(ds, job)]
        jobid = lsfdispatch.dispatch(funcName, keywords=keywords, lsfOptions=lsfOptions)
        msg = 'computeJobs(): starting job on grid.  lsfjobid={}, ds={}, job={}'.format(jobid, ds, job)
        print msg
        logging.log(roundup_common.ROUNDUP_LOG_LEVEL, msg)


def computeJob(ds, job):
    '''
    job: identifies which job this is so it knows which pairs to compute.
    computes orthologs for every pair in the job.  merges the orthologs into a single file and puts that file in the dataset orthologs dir.
    '''
    if isComplete(ds, 'job', job):
        return
    # a job is complete when all of its pairs are complete and it has written all the orthologs for all the pairs to a file.
    print ds
    print job
    pairs = getJobPairs(ds, job)
    print pairs
    jobDir = getJobDir(ds, job)
    print jobDir

    # compute orthologs for pairs
    for pair in pairs:
        orthologsPath = os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair))
        print orthologsPath
        computePair(ds, pair, jobDir, orthologsPath)

    # merge orthologs for pairs into a single file.
    if not isComplete(ds, 'job_ologs_merge', job):
        # merge orthologs into a single file
        removeJobOrthologs(ds, job) # remove any pre-existing stuff.  could happen if a job fails while writing orthologs to the file.
        for pair in pairs:
            orthologsPath = os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair))
            paramsAndOrthologs = readOrthologsFile(orthologsPath)
            addJobOrthologs(ds, job, paramsAndOrthologs)
        markComplete(ds, 'job_ologs_merge', job)
        
    # delete the individual pair olog files
    for pair in pairs:
        orthologsPath = os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair))
        if os.path.exists(orthologsPath):
            os.remove(orthologsPath)

    # mark the job complete
    markComplete(ds, 'job', job)
    
            
def computePair(ds, pair, workingDir, orthologsPath):
    '''
    ds: the roundup dataset.
    pair: find orthologs for this pair of genomes.
    workingDir: where to save blast hits and orthologs as they get completed.
    orthologsPath: where to write the orthologs.
    precompute blast hits for pair, then compute orthologs for pair, then write orthologs to a file and clean up other files.
    '''
    # a pair is complete when it has written its orthologs to a file and cleaned up its other data files.
    if isComplete(ds, 'pair', pair):
        return
    pairStartTime = time.time()
    queryGenome, subjectGenome = pair
    queryGenomePath = getGenomePath(ds, queryGenome)
    subjectGenomePath = getGenomePath(ds, subjectGenome)
    queryFastaPath = getGenomeFastaPath(ds, queryGenome)
    subjectFastaPath = getGenomeFastaPath(ds, subjectGenome)
    queryIndexPath = getGenomeIndexPath(ds, queryGenome)
    subjectIndexPath = getGenomeIndexPath(ds, subjectGenome)
    forwardHitsPath = os.path.join(workingDir, '{}_{}.forward_hits.pickle'.format(*pair))
    reverseHitsPath = os.path.join(workingDir, '{}_{}.reverse_hits.pickle'.format(*pair))
    maxEvalue = max([float(evalue) for evalue in roundup_common.EVALUES]) # evalues are strings like '1e-5'
    divEvalues = list(roundup_common.genDivEvalueParams())    
    with nested.NestedTempDir(dir=roundup_common.LOCAL_DIR) as tmpDir:
        logging.debug('computePair. tmpDir={}'.format(tmpDir))
        if not isComplete(ds, 'blast', queryGenome, subjectGenome):
            startTime = time.time()
            rsd.computeBlastHits(queryFastaPath, subjectIndexPath, forwardHitsPath, maxEvalue, workingDir=tmpDir, copyToWorking=roundup_common.ROUNDUP_LOCAL)
            putBlastStats(ds, queryGenome, subjectGenome, startTime=startTime, endTime=time.time())
            markComplete(ds, 'blast', queryGenome, subjectGenome)

        if not isComplete(ds, 'blast', subjectGenome, queryGenome):
            startTime = time.time()
            rsd.computeBlastHits(subjectFastaPath, queryIndexPath, reverseHitsPath, maxEvalue, workingDir=tmpDir, copyToWorking=roundup_common.ROUNDUP_LOCAL)
            putBlastStats(ds, subjectGenome, queryGenome, startTime=startTime, endTime=time.time())
            markComplete(ds, 'blast', subjectGenome, queryGenome)

        if not isComplete(ds, 'roundup', pair):
            startTime = time.time()
            divEvalueToOrthologs =  rsd.computeOrthologsUsingSavedHits(queryFastaPath, subjectFastaPath, divEvalues, forwardHitsPath, reverseHitsPath, workingDir=tmpDir)
            paramsAndOrthologs = [((queryGenome, subjectGenome, div, evalue), orthologs) for (div, evalue), orthologs in divEvalueToOrthologs.items()]
            writeOrthologsFile(paramsAndOrthologs, orthologsPath)
            # convert orthologs from a map to a table.
            # orthologs = []
            # for (div, evalue), partialOrthologs in divEvalueToOrthologs.items():
            #     for query, subject, distance in partialOrthologs:
            #         orthologs.append((queryGenome, subjectGenome, div, evalue, query, subject, distance))
            # writeOrthologsFile(orthologs, orthologsPath)
            putRsdStats(ds, queryGenome, subjectGenome, divEvalues, startTime=startTime, endTime=time.time())
            markComplete(ds, 'roundup', pair)
    # clean up files
    if os.path.exists(forwardHitsPath):
        os.remove(forwardHitsPath)
    if os.path.exists(reverseHitsPath):
        os.remove(reverseHitsPath)
    # complete pair computation
    putPairStats(ds, queryGenome, subjectGenome, startTime=pairStartTime, endTime=time.time())
    markComplete(ds, 'pair', pair)


#################################
# LOADING DATASET INTO DATABASE
#################################


def loadDatabase(ds, dropCreate=True, clean=True, makeIds=True, writeLookups=True, writeResults=True, writeSeqs=True, readSeqsMetadata=True, loadTables=True, loadResults=True):
    '''
    Drop and Create database tables for this dataset.
    Write files for each table that can be loaded with LOAD DATA INFILE.
    Why use LOAD DATA INFILE?  Because it is very fast relative to insert.  a discussion of insertion speed: http://dev.mysql.com/doc/refman/5.1/en/insert-speed.html
    Load the files into the tables.
    '''
    
    dsId = getDatasetId(ds)
    loadDir = getLoadDir(ds)

    if dropCreate:
        print 'dropping and creating tables'
        roundup_db.dropVersion(dsId)
        roundup_db.createVersion(dsId)

    if clean:
        print 'cleaning load dir'
        for path in glob.glob(os.path.join(loadDir, '*')):
            if os.path.isdir(path):
                shutil.rmtree(path)
            else:
                os.remove(path)

    if makeIds:
        print 'creating unique ids for genes, genomes, divs, and evalues'
        genomes = getGenomes(ds)
        genomeToId = dict([(genome, i) for i, genome in enumerate(genomes, 1)])
        print '...genomeToGenes'
        genomeToGenes = getData(ds, GENOME_TO_GENES)
        print '...genes'
        # only use genes from genomes used by dataset, not all genomes.  sometimes a dataset does not compute orthologs for all genomes in uniprot.
        genes = list(itertools.chain.from_iterable([genomeToGenes[genome] for genome in genomes])) 
        # genes = getData(ds, GENES)
        geneToId = dict([(gene, i) for i, gene in enumerate(genes, 1)])
        divs = roundup_common.DIVERGENCES
        divToId = dict([(div, i) for i, div in enumerate(divs, 1)])
        evalues = roundup_common.EVALUES
        evalueToId = dict([(evalue, i) for i, evalue in enumerate(evalues, 1)])

    genomesFile = os.path.join(loadDir, 'genomes.txt')
    divsFile = os.path.join(loadDir, 'divs.txt')
    evaluesFile = os.path.join(loadDir, 'evalues.txt')
    seqsFile = os.path.join(loadDir, 'seqs.txt')
    seqToGoTermsFile = os.path.join(loadDir, 'seqToGoTerms.txt')
    resultsFile = os.path.join(loadDir, 'results.txt') # wow this is going to be a big file!

    if writeLookups:
        print 'writing lookup tables'
        genomeToName = getMetadata(ds)['genomeToName']
        writeGenomesTable(genomes, genomeToId, genomeToName, genomesFile)
        writeLookupTable(divs, divToId, divsFile)
        writeLookupTable(evalues, evalueToId, evaluesFile)

    if writeResults:
        print 'writing results table'
        orthologsFiles = getOrthologsFiles(ds)
        writeResultsTable(genomeToId, divToId, evalueToId, geneToId, orthologsFiles, resultsFile)

    if loadResults:
        print 'loading results'
        roundup_db.loadVersionResults(dsId, genomeToId, divToId, evalueToId, geneToId, getParamsAndOrthologs(ds))

    if readSeqsMetadata:
        print 'reading in metadata about genomes, terms, and genes'
        # geneToData = getGeneToData(ds)
        print '...genomeToGenes'
        genomeToGenes = getData(ds, GENOME_TO_GENES)
        print '...geneToName'
        geneToName = getData(ds, GENE_TO_NAME)
        print '...geneToGenome'
        geneToGenome = getData(ds, GENE_TO_GENOME)
        print '...geneToGoTerms'
        geneToGoTerms = getData(ds, GENE_TO_GO_TERMS)
        print '...geneToGeneIds'
        geneToGeneIds = getData(ds, GENE_TO_GENE_IDS)
        print '...termToData'
        termToData = getData(ds, TERM_TO_DATA)

    if writeSeqs:
        print 'writing seqToGoTerms table'
        writeSeqToGoTermsTable(genes, geneToId, geneToGoTerms, termToData, seqToGoTermsFile)
        print 'writing seqs table'
        writeSeqsTable(genes, geneToId, geneToName, geneToGeneIds, geneToGenome, genomeToId, seqsFile)

    if loadTables:
        print 'loading tables'
        roundup_db.loadVersion(dsId, genomesFile, divsFile, evaluesFile, seqsFile, seqToGoTermsFile)


def writeResultsTable(genomeToId, divToId, evalueToId, geneToId, orthologsFiles, resultsFile):
    '''
    `id` int(10) unsigned NOT NULL auto_increment,
    `query_db` smallint(5) unsigned NOT NULL,
    `subject_db` smallint(5) unsigned NOT NULL,
    `divergence` tinyint(3) unsigned NOT NULL,
    `evalue` tinyint(3) unsigned NOT NULL,
    `filename` text,
    `mod_time` datetime default NULL,
    `orthologs` longblob,
    `num_orthologs` int(10) unsigned NOT NULL,
    '''
    with open(resultsFile, 'w') as fh:
        resultId = 0 # unique row id. 
        for path in orthologsFiles:
            for (qdb, sdb, div, evalue), orthologs in orthologsFileGen(path):
                # convert various items into the form the database table wants.  Change strings into database ids.  Encode orthologs, etc.
                qdbId = genomeToId[qdb]
                sdbId = genomeToId[sdb]
                divId = divToId[div]
                evalueId = evalueToId[evalue]
                fixedOrthologs = [(geneToId[qid], geneToId[sid], float(dist)) for qid, sid, dist in orthologs]
                resultId += 1
                numOrthologs = len(fixedOrthologs)
                jsonOrthologs = json.dumps(fixedOrthologs)
                fh.write('{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n'.format(resultId, qdbId, sdbId, divId, evalueId, 'NA', '\\N', jsonOrthologs, numOrthologs))


def writeSeqToGoTermsTable(genes, geneToId, geneToGoTerms, termToData, seqToGoTermsFile):
    '''
    `id` int(10) unsigned NOT NULL auto_increment,
    `sequence_id` int(10) unsigned NOT NULL,
    `go_term_acc` varchar(255) NOT NULL,
    `go_term_name` varchar(255) NOT NULL,
    `go_term_type` varchar(55) NOT NULL,
    '''
    with open(seqToGoTermsFile, 'w') as fh:
        i = 0 # unique row id
        for gene in genes:
            for term in geneToGoTerms[gene]:
                if term in termToData: # terms can be missing b/c term to data only contains biological_process, not molecular_function terms.
                    i += 1
                    fh.write('{}\t{}\t{}\t{}\t{}\n'.format(i, geneToId[gene], term, termToData[term][NAME], termToData[term][TYPE]))


def writeSeqsTable(genes, geneToId, geneToName, geneToGeneIds, geneToGenome, genomeToId, seqsFile):
    '''
    `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
    `external_sequence_id` varchar(100) NOT NULL,
    `genome_id` smallint(5) unsigned NOT NULL,
    `gene_name` varchar(20) DEFAULT NULL,
    `gene_id` int(11) DEFAULT NULL,      
    '''
    with open(seqsFile, 'w') as fh:
        for gene in genes:
            genomeId = genomeToId[geneToGenome[gene]]
            geneName = geneToName[gene]
            geneIds = geneToGeneIds[gene]
            geneId = geneIds[0] if geneIds else '\\N' # only use the first ncbi gene id if there are several. if no ids, use the mysql NULL character.
            fh.write('{}\t{}\t{}\t{}\t{}\n'.format(geneToId[gene], gene, genomeId, geneName, geneId))
                 

def writeGenomesTable(genomes, genomeToId, genomeToName, genomesFile):
    '''
    '''
    with open(genomesFile, 'w') as fh:
        for genome in genomes:
            fh.write('{}\t{}\t{}\n'.format(genomeToId[genome], genome, genomeToName[genome]))
    
            
def writeLookupTable(items, itemToId, itemsFile):
    '''
    write a file appropriate for loading into mysql.  each line contains a tab-separated id and item.
    '''
    with open(itemsFile, 'w') as fh:
        for item in items:
            fh.write('{}\t{}\n'.format(itemToId[item], item))
    
            
######
# JOBS
######

def getJobs(ds, refresh=False):
    '''
    caches jobs in the dataset metadata if they have not already been
    cached, b/c the isilon is wicked slow at listing dirs.
    returns: list of jobs in the dataset.
    '''
    if refresh:
        return updateMetadata(ds, {'jobs': os.listdir(getJobsDir(ds))})['jobs']
    else:
        jobs = getMetadata(ds).get('jobs')
        if not jobs:
            return updateMetadata(ds, {'jobs': os.listdir(getJobsDir(ds))})['jobs']
        else:
            return jobs


def getJobPairs(ds, job):
    with open(os.path.join(getJobDir(ds, job), 'job_pairs.json')) as fh:
        return json.load(fh)


def setJobPairs(ds, job, pairs):
    with open(os.path.join(getJobDir(ds, job), 'job_pairs.json'), 'w') as fh:
        json.dump(pairs, fh, indent=0)
    


def getJobDir(ds, job):
    return os.path.join(getJobsDir(ds), job)


def getJobOrthologsPath(ds, job):
    return os.path.join(getOrthologsDir(ds), '{}.orthologs.txt'.format(job))


def getJobOrthologs(ds, job):
    return readOrthologsFile(getJobOrthologsPath(ds, job))


def removeJobOrthologs(ds, job):
    writeOrthologsFile([], getJobOrthologsPath(ds, job))


def addJobOrthologs(ds, job, paramsAndOrthologs):
    writeOrthologsFile(paramsAndOrthologs, getJobOrthologsPath(ds, job), mode='a')


def getComputeJobName(ds, job):
    return getDatasetId(ds) + '_' + job


def isJobRunning(ds, job):
    '''
    checks if job is running on LSF.
    returns: True if job is on LSF and has not ended.  False otherwise.
    '''
    jobName = getComputeJobName(ds, job)
    infos = LSF.getJobInfosByJobName(jobName)
    statuses = [s for s in [info[LSF.STATUS] for info in infos] if not LSF.isEndedStatus(s)]
    if len(statuses) > 1:
        msg = 'isJobRunning: more than one non-ended LSF job for '+jobName+'\ndataset='+str(ds)+'\nstatuses='+str(statuses)
        raise Exception(msg)
    if statuses:
        return True
    else:
        return False
                                                        

#########
# GENOMES
#########

def getGenomes(ds):
    '''
    gets genomes cached in the metadata.
    returns: list of genomes in the dataset.
    '''
    return getMetadata(ds)['genomes']


def setGenomes(ds, genomes=None):
    '''
    genomes: a list of genomes to cache.  If None, will get the list from the genomes directory.
    caches genomes in the dataset metadata, b/c the isilon is wicked slow at listing dirs.
    also useful for testing.  can make a small dataset.
    '''
    if genomes is None:
        genomes = os.listdir(getGenomesDir(ds))
    return updateMetadata(ds, {'genomes': genomes})['genomes']

    
def getGenomesAndPaths(ds):
    '''
    returns: a dict mapping every genome in the dataset to its genomePath.
    '''
    genomesAndPaths = {}
    genomesDir = getGenomesDir(ds)
    for genome in getGenomes(ds):
        genomesAndPaths[genome] = os.path.join(genomesDir, genome)
    return genomesAndPaths


def getGenomePath(ds, genome):
    '''
    a genomePath is a directory containing genome fasta files and blast indexes.
    '''
    return os.path.join(getGenomesDir(ds), genome)


def getGenomeFastaPath(ds, genome):
    '''
    location of fasta file for a genome
    '''
    # .faa for fasta files containing amino acids.  http://en.wikipedia.org/wiki/FASTA_format#File_extension
    return os.path.join(getGenomePath(ds, genome), genome+'.faa')


def getGenomeIndexPath(ds, genome):
    '''
    location of blast index files.
    '''
    return os.path.join(getGenomePath(ds, genome), genome+'.faa')

    

###########
# ORTHOLOGS
###########


def getParamsAndOrthologs(ds):
    '''
    yields: a tuple pair of a tuple of params and a list of orthologs for every group of orthologs in the dataset.
    '''
    for path in getOrthologsFiles(ds):
        for params, orthologs in orthologsFileGen(path):
            yield params, orthologs
            
    
def orthologsFileGen(path):
    '''
    useful for iterating through the orthologs in a large file
    yields: a tuple pair of a tuple of params and a list of orthologs for every group of orthologs in path.
    '''
    if os.path.exists(path):
        with open(path) as fh:
            for line in fh:
                if line.startswith('PA'):
                    lineType, qdb, sdb, div, evalue = line.strip().split('\t')
                    orthologs = []
                elif line.startswith('OR'):
                    lineType, qid, sid, dist = line.strip().split('\t')                        
                    orthologs.append((qid, sid, dist))
                elif line.startswith('//'):
                    yield ((qdb, sdb, div, evalue), orthologs)


def readOrthologsFile(path):
    '''
    returns: a list of pairs of params and their associated orthologs for every group of orthologs in path
    '''
    return list(orthologsFileGen(path))


def writeOrthologsFile(paramsAndOrthologs, path, mode='w'):
    '''
    Inspired by the Uniprot dat files, a set of orthologs starts with a params row, then has 0 or more ortholog rows, then has an end row.
    Easy to parse.  Can represent a set of parameters with no orthologs.
    Format example:
    PA\tLACJO\tYEAS7\t0.2\t1e-15
    OR\tQ74IU0\tA6ZM40\t1.7016
    OR\tQ74K17\tA6ZKK5\t0.8215
    //
    PA      MYCGE   MYCHP   0.2     1e-15
    //
    '''
    with open(path, mode) as fh:
        for (qdb, sdb, div, evalue), orthologs in paramsAndOrthologs:
            fh.write('PA\t{}\t{}\t{}\t{}\n'.format(qdb, sdb, div, evalue))
            for ortholog in orthologs:
                fh.write('OR\t{}\t{}\t{}\n'.format(*ortholog))
            fh.write('//\n')


def getOrthologsFiles(ds):
    return glob.glob(os.path.join(getOrthologsDir(ds), '*.orthologs.txt'))


#######
# PAIRS
#######

def getPairs(ds, genomes=None):
    '''
    returns: a sorted list of pairs, where every pair is a sorted list of each combination of two genomes.
    '''
    if genomes is None:
        genomes = getGenomes(ds)
    return sorted(set([tuple(sorted((g1, g2))) for g1 in genomes for g2 in genomes if g1 != g2]))


###########
# COMPLETES
###########
# two different completes: one better for many completes and concurrency, the other for hand editing and associated with the dataset.

# isComplete uses the mysql database.  Used for concurrent execution of jobs, pairs, and other large numbers of completes.
# pros: concurrency. fast even with millions of completes.
# cons: different mysql db for dev and prod, so must use the prod code on a prod dataset.

def isComplete(ds, *key):
    return kvGet(ds, key=str(key), default=False, ns='completes')


def markComplete(ds, *key):
    kvPut(ds, key=str(key), value=True, ns='completes')


def resetCompletes(ds):
    resetKVStore(ds, ns='completes')    


def deleteCompletes(ds):
    deleteKVStore(ds, ns='completes')    


# isStepComplete uses a flat file in the dataset.  Used for downloading source files and other few completes executed serially.
# pros: easy to edit completes by hand.  completes associated with the dataset, not the database environment of the code base.  
# cons: very slow for many completes; concurrent writing of completes is unsafe.
def isStepComplete(ds, *key):
    stepsPath = os.path.join(ds, 'steps.complete.txt')
    if not os.path.exists(stepsPath):
        return False
    keyStr = str(key)
    with open(stepsPath) as fh:
        completes = set(line.strip() for line in fh if line.strip())
        return keyStr in completes

    
def markStepComplete(ds, *key):
    stepsPath = os.path.join(ds, 'steps.complete.txt')
    with open(stepsPath, 'a') as fh:
        fh.write(str(key)+'\n')
        
        
#################
# KEY-VALUE STORE
#################
# Uses a database backed key-value store.  This makes it fast, even with millions of rows, and concurrency-safe.

def kvGet(ds, key, default=None, ns='main'):
    return getKVStore(ds, ns).get(key, default)

    
def kvPut(ds, key, value, ns='main'):
    getKVStore(ds, ns).put(key, str(value))


KEY_VALUE_CACHE = {}
def getKVStore(ds, ns='main'):
    '''
    get the kvstore from the cache, or create one and place it in the cache.
    '''
    if ds in KEY_VALUE_CACHE and ns in KEY_VALUE_CACHE[ds]:
        return KEY_VALUE_CACHE[ds][ns]
    dsId = getDatasetId(ds)
    kv = kvstore.KVStore(util.ClosingFactoryCM(config.openDbConn), table='roundup_kvstore_{}_{}'.format(dsId, ns))
    KEY_VALUE_CACHE.setdefault(ds, {})[ns] = kv
    return kv


def resetKVStore(ds, ns='main'):
    '''
    ds: a dataset id
    ns: a namespace for the keys.  a string.  useful for separating key-values into groups that can be reset separately.
    drop and create the table for ns, thereby clearing all the keys and values in it.
    '''
    dsId = getDatasetId(ds)
    if ds in KEY_VALUE_CACHE and ns in KEY_VALUE_CACHE[ds]:
        del KEY_VALUE_CACHE.setdefault(ds, {})[ns]
    kv = kvstore.KVStore(util.ClosingFactoryCM(config.openDbConn), table='roundup_kvstore_{}_{}'.format(dsId, ns), drop=True, create=True)


def deleteKVStore(ds, ns='main'):
    '''
    used when deleting a dataset.
    '''
    dsId = getDatasetId(ds)
    if ds in KEY_VALUE_CACHE and ns in KEY_VALUE_CACHE[ds]:
        del KEY_VALUE_CACHE.setdefault(ds, {})[ns]
    kv = kvstore.KVStore(util.ClosingFactoryCM(config.openDbConn), table='roundup_kvstore_{}_{}'.format(dsId, ns), drop=True, create=False)
    

##################
# HELPER FUNCTIONS
##################

def makeUniqueDatetimeName(datetimeObj=None, prefix='', suffix=''):
    '''
    prefix: e.g. 'results_'
    suffix: e.g. '.txt'
    datetimeObj: provide a datetime object if do not want to use the current date and time.
    returns: a unique name stamped with and sortable by current date and time, e.g. 'results_20090212_155450_e547be40-bca8-4a98-8a3e-1ed923dd97de.txt'
    '''
    if not datetimeObj:
        datetimeObj = datetime.datetime.now()
    return prefix + datetimeObj.strftime("%Y%m%d_%H%M%S") + "_" + uuid.uuid4().hex + suffix


##########
# METADATA
##########
# The metadata of a dataset is persistent information describing the dataset.
# It is stored in a file, so is not safe for concurrent writing.

def updateMetadata(ds, metadata):
    '''
    metadata: a dict containing information about the dataset.  e.g. source files, download times, genome names.
    update existing dataset metadata with the values in metadata.
    returns: the updated dataset metadata.
    '''
    md = getMetadata(ds)
    md.update(metadata)
    return setMetadata(ds, md)

    
def getMetadata(ds):
    '''
    returns: a dict, the existing persisted dataset metadata.
    '''
    return getData(ds, DATASET)
    

def setMetadata(ds, metadata):
    return setData(ds, DATASET, metadata)


def getData(ds, key):
    '''
    used for storing big chunks of metadata, like geneToDesc.
    '''
    path = os.path.join(ds, 'metadata.{}.json'.format(key))
    with open(path) as fh:
        return json.load(fh)


def setData(ds, key, data):
    '''
    used for storing big chunks of metadata, like geneToDesc.
    '''
    path = os.path.join(ds, 'metadata.{}.json'.format(key))
    with open(path, 'w') as fh:
        json.dump(data, fh, indent=0)
    return data


#################
# STATS FUNCTIONS
#################


def resetStats(ds):
    resetKVStore(ds, ns='stats')    


def deleteStats(ds):
    deleteKVStore(ds, ns='stats')    


def getStats(ds, key):
    '''
    if key is not present, returns an empty dict
    '''
    return kvGet(ds, key=str(key), default={}, ns='stats')


def putStats(ds, key, stats):
    '''
    stats: a dict of statistics for blast, rsd, or a pair.
    '''
    kvPut(ds, key=str(key), value=stats, ns='stats')


def putBlastStats(ds, qdb, sdb, startTime, endTime):
    stats = {'type': 'blast', 'qdb': qdb, 'sdb': sdb, 'startTime': startTime, 'endTime': endTime}
    key = ('blast', qdb, sdb)
    putStats(ds, key, stats)
    return stats


def getBlastStats(ds, qdb, sdb):
    key = ('blast', qdb, sdb)
    return getStats(ds, key)

    
def putRsdStats(ds, qdb, sdb, divEvalues, startTime, endTime):
    stats = {'type': 'roundup', 'qdb': qdb, 'sdb': sdb, 'divEvalues': divEvalues, 'startTime': startTime, 'endTime': endTime}
    key = ('rsd', qdb, sdb)
    putStats(ds, key, stats)
    return stats


def getRsdStats(ds, qdb, sdb):
    key = ('rsd', qdb, sdb)
    return getStats(ds, key)

    
def putPairStats(ds, qdb, sdb, startTime, endTime):
    qdbFastaPath = getGenomeFastaPath(ds, qdb)
    sdbFastaPath = getGenomeFastaPath(ds, sdb)
    qdbBytes = os.path.getsize(qdbFastaPath)
    sdbBytes = os.path.getsize(sdbFastaPath)
    genomeToCount = getMetadata(ds)['genomeToCount']
    qdbSeqs = genomeToCount[qdb]
    sdbSeqs = genomeToCount[sdb]
    stats = {'type': 'pair', 'qdb': qdb, 'sdb': sdb, 'startTime': startTime, 'endTime': endTime,
             'qdbBytes': qdbBytes, 'sdbBytes': sdbBytes, 'qdbSeqs': qdbSeqs, 'sdbSeqs': sdbSeqs}
    key = ('pair', qdb, sdb)
    putStats(ds, key, stats)
    return stats


def getPairStats(ds, qdb, sdb):
    key = ('pair', qdb, sdb)
    return getStats(ds, key)
    


########################
# DEPRECATED / UNUSED
########################


def getGenes(ds):
    '''
    return: a list of every seq id in the genome fasta files.  the seq ids are all unique in the list.
    '''
    return getMetadataSub(ds, 'genes.json', [])

    
def setGenes(ds, genes):
    return setMetadataSub(ds, 'genes.json', genes)


def getGeneToData(ds):
    return getMetadataSub(ds, 'gene_to_data.json', [])


def setGeneToData(ds, geneToData):
    return setMetadataSub(ds, 'gene_to_data.json', geneToData)


def getGeneToGenome(ds):
    return getMetadataSub(ds, 'gene_to_genome.json', [])


def setGeneToGenome(ds, geneToGenome):
    return setMetadataSub(ds, 'gene_to_genome.json', geneToGenome)


def getGeneToName(ds):
    return getMetadataSub(ds, 'gene_to_name.json', [])


def setGeneToName(ds, geneToName):
    return setMetadataSub(ds, 'gene_to_name.json', geneToName)


def getGeneToDesc(ds):
    return getMetadataSub(ds, 'gene_to_desc.json', [])


def setGeneToDesc(ds, geneToDesc):
    return setMetadataSub(ds, 'gene_to_desc.json', geneToDesc)


def getGeneToGeneIds(ds):
    return getMetadataSub(ds, 'gene_to_geneids.json', [])


def setGeneToGeneIds(ds, geneToGeneIds):
    return setMetadataSub(ds, 'gene_to_geneids.json', geneToGeneIds)


def getGeneToGoTerms(ds):
    return getMetadataSub(ds, 'gene_to_go_terms.json', [])


def setGeneToGoTerms(ds, geneToGoterms):
    return setMetadataSub(ds, 'gene_to_go_terms.json', geneToGoTerms)


def getMetadataSubOld(ds, filename, default):
    path = os.path.join(ds, filename)
    if os.path.exists(path):
        with open(path) as fh:
            return json.load(fh)
    else:
        return default
    

def writeResultsTableOld(genomeToId, divToId, evalueToId, geneToId, orthologsFiles, resultsFile):
    '''
    `id` int(10) unsigned NOT NULL auto_increment,
    `query_db` smallint(5) unsigned NOT NULL,
    `subject_db` smallint(5) unsigned NOT NULL,
    `divergence` tinyint(3) unsigned NOT NULL,
    `evalue` tinyint(3) unsigned NOT NULL,
    `filename` text,
    `mod_time` datetime default NULL,
    `orthologs` longblob,
    `num_orthologs` int(10) unsigned NOT NULL,
    '''
    with open(resultsFile, 'w') as fh:
        resultId = 0
        for path in orthologsFiles:
            with open(path) as fh2:
                qdbId = sdbId = divId = evalueId = None
                orthologs = []
                for line in fh2:
                    # example line: LACJO   YEAS7   0.2     1e-15   Q74IU0  A6ZM40  1.7016
                    qdb, sdb, div, evalue, qid, sid, dist = line.split('\t')
                    newQdbId = genomeToId[qdb]
                    newSdbId = genomeToId[sdb]
                    newDivId = divToId[div]
                    newEvalueId = evalueToId[evalue]
                    if (qdbId != newQdbId or sdbId != newSdbId or divId != newDivId or evalueId != newEvalueId):
                        if orthologs:
                            resultId += 1
                            numOrthologs = len(orthologs)
                            encodedOrthologs = roundup_db.encodeOrthologs(orthologs)
                            fh.write('{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n'.format(resultId, qdbId, sdbId, divId, evalueId, 'NA', '\\N', encodedOrthologs, numOrthologs))
                        qdbId = newQdbId
                        sdbId = newSdbId
                        divId = newDivId
                        evalueId = newEvalueId
                        orthologs = []
                    dist = float(dist)
                    orthologs.append((geneToId[qid], geneToId[sid], dist))
                if orthologs:
                    resultId += 1
                    numOrthologs = len(orthologs)
                    encodedOrthologs = json.dumps(orthologs) # roundup_db.encodeOrthologs(orthologs)
                    fh.write('{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n'.format(resultId, qdbId, sdbId, divId, evalueId, 'NA', '\\N', encodedOrthologs, numOrthologs))
                    
                    

def getNewPairs(ds):
    return readPairsFile(os.path.join(ds, 'new_pairs.txt'))


def setNewPairs(ds, pairs):
    writePairsFile(pairs, os.path.join(ds, 'new_pairs.txt'))


def getOldPairs(ds):
    return readPairsFile(os.path.join(ds, 'old_pairs.txt'))


def setOldPairs(ds, pairs):
    writePairsFile(pairs, os.path.join(ds, 'old_pairs.txt'))


def writePairsFile(pairs, path):
    with open(path, 'w') as fh:
        json.dump(pairs, fh, indent=2)
            

def readPairsFile(path):
    pairs = []
    if os.path.exists(path):
        with open(path) as fh:
            pairs = json.load(fh)
    return pairs



# CONVERSION OF ORTHOLOGS.TXT FILES FROM OLD TO NEW FORMAT
# old format: tab-separated qdb, sdb, div, evalue, qid, sid, and dist.
# example line: LACJO   YEAS7   0.2     1e-15   Q74IU0  A6ZM40  1.7016
# all lines are the same.
# pros: very simple.  each line describes all the params.
# cons: bigger files.  can not represent a pair of genomes (and div, evalue) that has no orthologs.  also, a bit of a pain to parse into groups of orthologs

# new format:
# a set of orthologs starts with a params row, then has 0 or more ortholog rows, then has an end row.
# Like this:
# PA\tLACJO\tYEAS7\t0.2\t1e-15
# OR\tQ74IU0\tA6ZM40\t1.7016
# OR\tQ74K17\tA6ZKK5\t0.8215
# //
# pros: smaller files.  easier to parse.  can represent a set of parameters with no orthologs.
# cons: not all rows are identical in format, so requires a stateful parser.  but that is required anyway.
    
def convertOrthologs(ds):
    convertedDir = os.path.join(ds, 'converted')
    if not os.path.exists(convertedDir):
        os.mkdir(convertedDir)
    orthologsFiles = getOrthologsFiles(ds)
    oldAndNewFiles = [(path, os.path.join(convertedDir, os.path.basename(path))) for path in orthologsFiles]
    # convertOrthologsFiles(oldAndNewFiles)
    # parallelize on lsf, b/c takes about 7 seconds per file and I have 4000 files.
    funcName = 'roundup_dataset.convertOrthologsFiles'
    for pairsList in util.splitIntoN(oldAndNewFiles, 200):
        keywords = {'oldAndNewFiles': pairsList}
        # convertOrthologsFiles(pairsList)
        print lsfdispatch.dispatch(funcName, keywords=keywords)

    
def convertOrthologsFiles(oldAndNewFiles):
    for path, newPath in oldAndNewFiles:
        print path, '=>', newPath
        convertOrthologsFile(path, newPath)

        
def convertOrthologsFile(path, newPath):
    START = True
    qdb = sdb = div = evalue = None
    orthologs = []
    with open(newPath, 'w') as fh, open(path) as fh2:
        for line in fh2:
            # example line: LACJO   YEAS7   0.2     1e-15   Q74IU0  A6ZM40  1.7016
            qdb2, sdb2, div2, evalue2, qid, sid, dist = line.strip().split('\t')
            if START: # start a new group of orthologs.
                qdb = qdb2
                sdb = sdb2
                div = div2
                evalue = evalue2
                orthologs = [(qid, sid, dist)]
                START = False
            elif qdb != qdb2 or sdb != sdb2 or div != div2 or evalue != evalue2: # a new group of orthologs.  write out last group and start a new group.
                fh.write('PA\t{}\t{}\t{}\t{}\n'.format(qdb, sdb, div, evalue))
                for ortholog in orthologs:
                    fh.write('OR\t{}\t{}\t{}\n'.format(*ortholog))
                fh.write('//\n')
                qdb = qdb2
                sdb = sdb2
                div = div2
                evalue = evalue2                    
                orthologs = [(qid, sid, dist)]
            else: # append to the current group of orthologs
                orthologs.append((qid, sid, dist))
        if qdb is not None: # at end of file, write out the current group (if any) of orthologs
            fh.write('PA\t{}\t{}\t{}\t{}\n'.format(qdb, sdb, div, evalue))
            for ortholog in orthologs:
                fh.write('OR\t{}\t{}\t{}\n'.format(*ortholog))
            fh.write('//\n')
            
            


# last line - python emacs bug fix
 
