#!/usr/bin/env python

'''
## CONCEPTS YOU NEED TO KNOW TO UNDERSTAND THIS CODE

* Dataset: ds, the id of a dataset.  also happens to be a directory path, but that is an implementation detail.
* Sources: a dir containing all the files needed to create the genomes and the metadata about those genomes: gene names, go terms, gene descriptions, genome names, etc.
* Genomes: A dir containing dirs named after each genome and containing a fasta file and blast indexes named after the genome.
  a genome id is an uniprot species identifier.  http://ca.expasy.org/sprot/userman.html#ID_line
* Jobs: a dir containing one dir for each job.  a job is the computational unit used for parallelism.  a job is assigned pairs to run rsd on.
  it runs rsd and stores the orthologs in the orthologs dir in a file named for the job.
* Orthologs: a dir containing files of orthologs generated by rsd.  
* Pair: a tuple of a pair of genomes, the "query" genome and "subject" genome.  a well-formed pair is sorted.
* Metadata: caches information about a dataset for quick retrieval.  
* Completes: track which stages of creating a dataset or running a computation have been completed.  There are two completes, one in the database that is concurrency-safe
  and scalable to millions of entries (useful for tracking complete pairs), and one on the filesystem, which is human editable and stored with the dataset so it is not
  sensitive to which code (dev or prod) is run.


'''

# # prepare a computation using only a few small genomes
# cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c "import roundup_dataset
# ds = '/groups/cbi/td23/test_dataset'
# orgCodes = 'MYCGE MYCGF MYCGH MYCH1 MYCH2 MYCH7 MYCHH MYCHJ MYCHP'.split()
# genomes = '243273 708616 710128 907287 295358 262722 872331 262719 347256'.split()
# roundup_dataset.prepareDataset(ds)
# roundup_dataset.setGenomes(ds, genomes)
# roundup_dataset.prepareJobs(ds, numJobs=10)
# "

# taxon used for genome b/c orgCode can be used for multiple organisms (e.g. subspecies).
# orgName used for genomeName

# A description of Uniprot Knowledgebase dat files:
# http://web.expasy.org/docs/userman.html
# "mnemonic species identification code"

# A list of all species ids abbreviations, their kingdom, taxon id, and official names, common names, and synonyms.  Cool!
# http://www.uniprot.org/docs/speclist
# "organism (species) identification code", "organism code"

# A description of how the uniprot taxonomy is organized, with explanation of organism codes.
# "mnemonic organism identification code"
# http://www.uniprot.org/help/taxonomy


# standard library modules
import argparse
import collections
import datetime
import glob
import itertools
import json
import logging
import os
import random
import re
import shutil
import subprocess
import sys
import time

# our modules
import config
import dbutil
import fasta
import kvstore
import nested
import orthoxml
import orthutil
import roundup_common
import rsd
import uniprot
import util
import workflow


MIN_GENOME_SIZE = 200 # ignore genomes with fewer sequences
DIR_MODE = 0775 # directories in this dataset are world readable and group writable.

# keys used for termToData and taxonToData  
NAME = 'name' # short to save space. boo.
TYPE = 'type' # short key to save space. boo
CAT_NAME = 'cat_name'
CAT_CODE = 'cat_code'

# METADATA STORES, keys for getData()
DATASET = 'dataset'
GENES = 'genes' 
GENE_TO_GENOME = 'gene_to_genome'
GENE_TO_NAME = 'gene_to_name'
GENE_TO_DESC = 'gene_to_desc'
GENE_TO_GO_TERMS = 'gene_to_go_terms'
GENE_TO_GENE_IDS = 'gene_to_gene_ids'
TERM_TO_DATA = 'term_to_data'
GENOME_TO_GENES = 'genome_to_genes'
TAXON_TO_DATA = 'taxon_to_data'
BLAST_STATS = 'blast_stats'
RSD_STATS = 'rsd_stats'
CHANGE_LOG = 'change_log'


def main(ds):
    '''
    '''
    print 'See commands.txt for details on how to make a dataset.'

        

#######################
# MAIN DATASET PIPELINE
#######################

def prepareDataset(ds):
    '''
    Make the directory structure, tables, etc., a new dataset needs.
    '''
    resetCompletes(ds) # make an empty db table for dones
    resetStats(ds) # make an empty db table for stats
    for path in [ds, getGenomesDir(ds), getOrthologsDir(ds), getJobsDir(ds),
                 getSourcesDir(ds), getDownloadDir(ds)]:
        if not os.path.exists(path):
            os.makedirs(path, DIR_MODE)
    setMetadata(ds, {}) # initialize a blank metadata for the dataset


def downloadSources(ds):
    '''
    Download uniprot files contained in an archive of a uniprot release.
    Download ncbi taxon categories file.
    Download go database.
    Save source urls, but fudge the uniprot ones to look like the archive url, b/c the "current" urls
    are not stable.
    '''
    
    print 'downloadSources: {}'.format(ds)
    sourcesDir = getSourcesDir(ds)

    currentUrl = 'ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete'
    uniprotFiles = ['reldate.txt', 'uniprot_sprot.dat.gz', 'uniprot_trembl.dat.gz']
    uniprotUrlDests = [(currentUrl + '/' + f, os.path.join(sourcesDir, 'uniprot', f)) for f in uniprotFiles]

    taxonUrlDests = [('ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxcat.tar.gz', os.path.join(sourcesDir, 'ncbi', 'taxcat.tar.gz')),
                     ('ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz', os.path.join(sourcesDir, 'ncbi', 'taxdump.tar.gz'))]
    
    goUrlDests = [('ftp://ftp.geneontology.org/pub/go/godatabase/archive/go_daily-termdb-tables.tar.gz',
                   os.path.join(sourcesDir, 'geneontology/go_daily-termdb-tables.tar.gz'))]
    
    urlDests = uniprotUrlDests + taxonUrlDests + goUrlDests
    print urlDests
    for url, dest in urlDests:
        downloadSource(ds, url, dest)

    # pretend like the uniprot source was the "archive" url, not the "current" urls.
    release = extractUniprotRelease(ds) # e.g. 2011_09
    uniprotArchiveUrl = 'ftp://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-{0}/knowledgebase/knowledgebase{0}.tar.gz'.format(release)

    sources = [uniprotArchiveUrl] + [url for url, dest in taxonUrlDests + goUrlDests]
    print sources
    updateMetadata(ds, {'sources': sources})
    print 'done downloading sources.'


def downloadSource(ds, url, dest):
    '''
    helper function.  downloads a source and marks it complete
    '''
    if not os.path.exists(os.path.dirname(dest)):
        os.makedirs(os.path.dirname(dest), DIR_MODE)
    print 'downloading {} to {}...'.format(url, dest)
    if isComplete(ds, 'download', url):
        print '...skipping because already downloaded.'
        return
    cmd = 'curl --remote-time --output '+dest+' '+url
    subprocess.check_call(cmd, shell=True)
    print
    markComplete(ds, 'download', url)
    print '...done.'
    pause = 10
    print 'pausing for {} seconds.'.format(pause)
    time.sleep(pause)
    

def processSources(ds):
    '''
    gunzip (and untar) some source files.
    '''
    print 'processing sources...'
    sourcesDir = getSourcesDir(ds)
    
    uniprotFiles = [
             'uniprot/uniprot_sprot.dat.gz', 
             'uniprot/uniprot_trembl.dat.gz', 
             ]
    taxonFiles = ['ncbi/taxcat.tar.gz', 'ncbi/taxdump.tar.gz']
    goFiles = ['geneontology/go_daily-termdb-tables.tar.gz']

    for f in uniprotFiles + taxonFiles + goFiles:
        path = os.path.join(sourcesDir, f)
        if isComplete(ds, 'process source', path):
            print '...skipping processing {} because already done.'.format(path)
            continue
        print 'processing', path
        if path.endswith('.tar.gz'):
            print '...tar xzf file'
            subprocess.check_call(['tar', '-xzf', path], cwd=os.path.dirname(path))
        elif path.endswith('.gz') and os.path.exists(path):
            print '...gunzip file'
            subprocess.check_call(['gunzip', path])
        markComplete(ds, 'process source', path)
            
    print '...done'


def extractUniprotRelease(ds):
    '''
    e.g. 2011_06
    '''
    release = uniprot.parseRelease(os.path.join(getSourcesDir(ds), 'uniprot/reldate.txt'))
    print 'release', release
    updateMetadata(ds, {'uniprotRelease': release})
    return release
    

def extractTaxonData(ds):
    '''
    taxon category codes and names from ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxcat_readme.txt:
    A = Archaea
    B = Bacteria
    E = Eukaryota
    V = Viruses and Viroids
    U = Unclassified and Other
    '''
    sourcesDir = getSourcesDir(ds)
    catCodeToName = {'A': 'Archaea', 'B': 'Bacteria', 'E': 'Eukaryota', 'V': 'Viruses and Viroids', 'U': 'Unclassified and Other'}
    taxonToData = collections.defaultdict(dict)
    for line in open(os.path.join(sourcesDir, 'ncbi/categories.dmp')):
        cat, speciestaxon, taxon = line.split()
        taxonToData[taxon][CAT_CODE] = cat
        taxonToData[taxon][CAT_NAME] = catCodeToName[cat]
    for line in open(os.path.join(sourcesDir, 'ncbi/names.dmp')):
        splits = [s.strip() for s in line.split('|')]
        if not len(splits) == 5:
            raise Exception('Wrong number of fields', splits, line)
        taxon, name, uniqueName, nameClass, other = splits
        if nameClass == 'scientific name':
            taxonToData[taxon][NAME] = name
    setData(ds, TAXON_TO_DATA, taxonToData)

        
def extractFromGeneOntology(ds):
    '''
    creates a lookup for go term accessions to name and term_type.  e.g. 'GO:2000779' => 'regulation of double-strand break repair', 'biological_process'    
    '''
    # `id` int(11) NOT NULL AUTO_INCREMENT,
    # `name` varchar(255) NOT NULL DEFAULT '',
    # `term_type` varchar(55) NOT NULL,
    # `acc` varchar(255) NOT NULL,
    # `is_obsolete` int(11) NOT NULL DEFAULT '0',
    # `is_root` int(11) NOT NULL DEFAULT '0',
    # `is_relation` int(11) NOT NULL DEFAULT '0',
    
    termToData = {}
    with open(os.path.join(getSourcesDir(ds), 'geneontology/go_daily-termdb-tables/term.txt')) as fh:
        for line in fh:
            id, name, termType, acc, isObsolete, etc = line.strip().split('\t', 5)
            if isObsolete == '0' and termType == 'biological_process':
                termToData[acc] = {NAME: name, TYPE: termType}
    setData(ds, TERM_TO_DATA, termToData)


def examineDats(ds, dats=None, surprisesFile=None, countsFile=None):
    '''
    ds: a dataset (dir)
    dats: a list of dat file paths.  Defaults to the dat files in the dataset.
    surprisesFile: where to write all the ways the dat files surprised the parser by being formed in unexpected ways.
    countsFile: where to write the count of sequences for each genome, including total, complete, and reference counts.
    Investigate which genomes we should include in roundup.
    Gather data about the sequences and genomes in the dat files, primarily counts per genome of total sequences, complete sequences,
    reference sequences, seqs in sprot, seqs in trembl.
    Check for surprises, like having >1 taxon or genome name or organism code per genome.
    This function was used to better understand the dat files, to decide whether or not to just use 'Complete proteome' seqs, and to
    see which genomes are new (not in last roundup).
    '''
    if not dats:
        dats = getDats(ds)
    if not surprisesFile:
        surprisesFile = os.path.join(ds, 'dat_surprises.txt')
    if not countsFile:
        countsFile = os.path.join(ds, 'dat_genome_counts.txt')
    print 'dats:', dats
    print 'surprisesFile:', surprisesFile
    print 'countsFile:', countsFile
    
    genomeToCount = collections.defaultdict(int) # maps each genome to the number of sequences (in the dat files) for that genome.
    genomeToCompleteCount = collections.defaultdict(int) # track number of seqs in the Complete proteome set of an organism
    genomeToReferenceCount = collections.defaultdict(int) # track number of seqs in the Reference proteome set of an organism
    genomeToSprotCount = collections.defaultdict(int) # track number of seqs from Swissprot
    genomeToTremblCount = collections.defaultdict(int) # track number of seqs from Trembl
    genomeToNames = collections.defaultdict(set)
    genomeToTaxons = collections.defaultdict(set)
    genomeToOrgCodes = collections.defaultdict(set) # maps each genome to a uniprot organism (species) id code. e.g. ECOLI, HUMAN, XENTR

    # collect exceptions to my assumptions about dat files
    # are genes or genomes missing something they should have?
    # do genes have more than one gene name, description?
    # do genomes have more than one version of a name, a taxon, a complete status?
    allSurprises = []
    
    for path in dats:
        for entryNum, data in enumerate(uniprot.genDatEntries(path)):
            ns, gene, orgCode, orgName, taxon, geneName, geneDesc, complete, reference, geneIds, goTerms, fastaLines, surprises = data
            allSurprises.extend(surprises)

            # taxon used for genome b/c orgCode can be used for multiple organisms (e.g. subspecies).
            # orgName used for genomeName
            assert taxon
            genome = taxon

            # Ignore "9" org code seqs b/c they are not a species or subspecies. http://www.uniprot.org/help/taxonomy
            if orgCode[0] == '9':
                continue

            genomeToOrgCodes[genome].add(orgCode)
            genomeToNames[genome].add(orgName)
            genomeToTaxons[genome].add(taxon)
            genomeToCount[genome] += 1
            if complete:
                genomeToCompleteCount[genome] += 1
            if reference:
                genomeToReferenceCount[genome] += 1
            if ns == 'sp':
                genomeToSprotCount[genome] += 1
            if ns == 'tr':
                genomeToTremblCount[genome] += 1

    countList = []
    countData = {}
    for genome in genomeToCount:
        # surprise! genome has multiple org codes
        if len(genomeToOrgCodes[genome]) > 1:
            allSurprises.append('\t'.join(('multiple_genome_org_codes', genome, ' '.join(genomeToOrgCodes[genome]))))
        # surprise! genome has multiple names
        if len(genomeToNames[genome]) > 1:
            allSurprises.append('\t'.join(('multiple_genome_names', genome, ' '.join(genomeToNames[genome])))) 
        # surprise! genome has multiple taxons
        if len(genomeToTaxons[genome]) > 1:
            allSurprises.append('\t'.join(('multiple_genome_taxons', genome, ' '.join(genomeToTaxons[genome])))) 
            
        refCount = genomeToReferenceCount[genome]
        compCount = genomeToCompleteCount[genome]
        if refCount and refCount != compCount:
            # surprise! different number of seqs in reference proteome than in complete proteome
            allSurprises.append('\t'.join(('reference_count_with_different_complete_count', '', genome, '', '', 
                                            'reference count: {}, complete count: {}'.format(refCount, compCount))))
        data = {'genome': genome, 'name': list(genomeToNames[genome])[0], 'taxon': list(genomeToTaxons[genome])[0],
                'org_code': list(genomeToOrgCodes[genome])[0],
                'count': genomeToCount[genome], 'complete_count': compCount, 'reference_count': refCount,
                'sprot_count': genomeToSprotCount[genome], 'trembl_count': genomeToTremblCount[genome],
                'num_names': len(genomeToNames[genome]), 'num_taxons': len(genomeToTaxons[genome]),
                'num_org_codes': len(genomeToOrgCodes[genome])}
        countData[genome] = data
        countList.append([data['count'], data['complete_count'], data['reference_count'], data['sprot_count'], data['trembl_count'],
                          data['genome'], data['org_code'], data['name'], data['num_names'], data['num_taxons'], data['num_org_codes']])
        
    with open(countsFile, 'w') as fh:
        for data in sorted(countList):
            fh.write('\t'.join(str(d) for d in data) + '\n')

    with open(surprisesFile, 'w') as fh:
        for surprise in allSurprises:
            fh.write(surprise + '\n')

    setData(ds, 'dat_surprises', allSurprises)
    setData(ds, 'dat_genome_counts', countData)

    print 'all done.', datetime.datetime.now()


def setGenomesFromCounts(ds):
    '''
    Use to set the genomes from uniprot that we are interested in.  These are genomes who have >= MIN_GENOME_SIZE
    sequences marked 'Complete proteome'.
    '''
    # we are interested in genomes with complete counts >= MIN_GENOME_SIZE
    countData = getData(ds, 'dat_genome_counts')
    genomes = [g for g in countData if countData[g]['complete_count'] >= MIN_GENOME_SIZE]
    setGenomes(ds, genomes)
    
    
def findMissingTaxonToData(ds):
    '''
    Report which genomes (of the genomes we are interested in) are missing from taxonToData or do not
    have all the data they should.
    Sadly, some taxon ids for uniprot genomes are missing from the ncbi taxonToData (category.dmp, names.dmp).
    Why are they missing?  Some are for organisms (e.g. virus isolates like VAR66) not in ncbi.
    For others, uniprot uses the wrong taxon id (e.g. SALPB).  Also, sometimes NCBI category.dmp disagrees
    with names.dmp, perhaps b/c one taxon can be replaced with another.  E.g. 662066 by 1111464
    This prints out which are missing.
    Then one should construct by hand a lookup table for the missing taxons and use
    setMissingTaxonToData() to create a lookup to be used when the database is loaded.
    '''
    genomes = getGenomes(ds)
    taxonToData = getTaxonToData(ds)
    for genome in genomes:
        taxon = genome
        if taxon not in taxonToData:
            print 'missing_taxon', taxon
        else:
            name = taxonToData[taxon].get(NAME)
            catCode = taxonToData[taxon].get(CAT_CODE)
            catName = taxonToData[taxon].get(CAT_NAME)
            if None in (name, catCode, catName):
                print 'missing_data', taxon, name, catCode, catName


def setMissingTaxonToData(ds, missingTaxonToData):
    '''
    update taxonToData with data for the genome taxons that were missing data.
    # example of updating taxonToData with missing taxon data
    cd /www/dev.roundup.hms.harvard.edu/webapp && time python -c "import roundup_dataset;
    ds = '/groups/cbi/roundup/datasets/3'
    missingData = {
    '654926': {roundup_dataset.NAME: 'Ectocarpus siliculosus virus 1 (isolate New Zealand/Kaikoura/1988)',
    roundup_dataset.CAT_CODE: 'V',
    roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
    '654925': {roundup_dataset.NAME: 'Emiliania huxleyi virus 86 (isolate United Kingdom/English Channel/1999)',
    roundup_dataset.CAT_CODE: 'V',
    roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
    '1054147': {roundup_dataset.NAME: 'Dictyostelium fasciculatum (strain SH3)',
    roundup_dataset.CAT_CODE: 'E',
    roundup_dataset.CAT_NAME: 'Eukaryota'},
    '587202': {roundup_dataset.NAME: 'Variola virus (isolate Human/Japan/Yamada MS-2(A)/1946)',
    roundup_dataset.CAT_CODE: 'V',
    roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
    '587203': {roundup_dataset.NAME: 'Variola virus (isolate Human/Brazil/v66-39/1966)',
    roundup_dataset.CAT_CODE: 'V',
    roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
    '587201': {roundup_dataset.NAME: 'Variola virus (isolate Human/South Africa/102/1965)',
    roundup_dataset.CAT_CODE: 'V',
    roundup_dataset.CAT_NAME: 'Viruses and Viroids'},
    }
    roundup_dataset.setMissingTaxonToData(ds, missingTaxonToData)
    "
    '''
    taxonToData = getTaxonToData(ds)
    taxonToData.update(missingTaxonToData)
    setData(ds, TAXON_TO_DATA, taxonToData)


def extractFromDats(ds, dats=None, writing=True, cleanDirs=False, bufSize=5000000):
    '''
    ds: a dataset (dir)
    dats: a list of dat file paths.  Defaults to the dat files in the dataset.
    writing: debugging. if False, fasta files will not be writen. Useful for debugging
    cleanDirs: optimization.  if True, all fasta files in the dataset will be removed before splitting the genomes, which takes time.
    bufSize: number of fasta sequence to cache in memory before writing to files.  this is to avoid writing frequently to
      files, b/c opening and closing files on Isilon fileserver is slow.
    Gather data about each 'Complete proteome' sequence, including name, description, go terms, ncbi gene ids.  Gather data about each
    associated genome, including name, ncbi taxon id, and sequence counts.  Create fasta files containing the seqs, one for each genome,
    trying to mimic the fasta name lines in the fasta files from uniprot.
    '''

    if not dats:
        dats = getDats(ds)
    print 'dats:', dats
            
    # remove any pre-existing genomes.
    if cleanDirs:
        print 'cleaning genomes directory...', datetime.datetime.now()
        cleanGenomes(ds)

    # helper function to create genome dirs for genomes not yet seen
    # fasta files placed in genome dirs
    dirGenomes = set()

    def makeGenomeDir(genome):
        if genome not in dirGenomes:
            dirGenomes.add(genome)
            genomePath = getGenomePath(ds, genome)
            if not os.path.exists(genomePath):
                os.mkdir(genomePath, DIR_MODE)

    # helper function to write fasta lines to a genome file.
    writeGenomes = set()

    def writeToGenome(genome, data):
        ''' data: a list of fasta lines, including newlines '''
        if not writing: # speed performance when not testing behavior that does not involve writing fasta
            return        
        if genome not in writeGenomes: # first time writing to the genome
            makeGenomeDir(genome) # make genome dir if missing
            mode = 'w'
            writeGenomes.add(genome)
        else:
            mode = 'a'
        with open(getGenomeFastaPath(ds, genome), mode) as outfh:
            output = ''.join(data)
            outfh.write(output)

    # use count data to avoid rescanning dat files.
    countData = getData(ds, 'dat_genome_counts')
    taxonToData = getTaxonToData(ds)
    
    genomes = getGenomes(ds)
    genomeToName = {g: taxonToData[g][NAME] for g in genomes} # use taxon name as genome name.
    genomeToCount = {g: countData[g]['complete_count'] for g in genomes}
    genomeToTaxon = {g: g for g in genomes}
    genomeToOrgCode = {g: countData[g]['org_code'] for g in genomes}

    # paranoid check:  No genome should have >1 name, taxon, or org code.
    for g in genomes:
        assert countData[g]['num_names'] == 1 and countData[g]['num_taxons'] == 1 and countData[g]['num_org_codes'] == 1

    # paranoid check: All genome names should be unique
    assert len(genomeToName.values()) == len(set(genomeToName.values()))
    
    geneToGenome = {} # track which sequences belong to which genome.  store sequences of all complete genomes
    geneToName = {}
    geneToDesc = {}
    geneToGeneIds = {}
    geneToGoTerms = {}
    genomeToGenes = collections.defaultdict(list)
    genomeToLines = collections.defaultdict(list) # buffer fasta sequence lines for output

    # Pass 2: collect data about genes and genomes, but only for genomes that are not too small.
    for path in dats:
        for entryNum, data in enumerate(uniprot.genDatEntries(path)):
            ns, gene, orgCode, orgName, taxon, geneName, geneDesc, complete, reference, geneIds, goTerms, fastaLines, surprises = data

            # taxon used for genome b/c orgCode can be used for multiple organisms (e.g. subspecies).
            genome = taxon

            # Ignore "9" org code seqs b/c they are not a species or subspecies. http://www.uniprot.org/help/taxonomy
            # Ignore seqs not marked "Complete proteome"
            # Ignore seqs from too small genomes
            if orgCode[0] == '9' or not complete or genome not in genomeToCount:
                continue
            
            geneToGenome[gene] = genome
            geneToName[gene] = geneName
            geneToDesc[gene] = geneDesc
            geneToGeneIds[gene] = geneIds
            geneToGoTerms[gene] = goTerms
            genomeToGenes[genome].append(gene)
            genomeToLines[genome].extend(fastaLines)

            # collect fasta lines for writing to fasta files
            if entryNum % bufSize == 0:
                # write out collected lines to the various genome fasta files
                print 'writing collected lines to fasta files for {} genomes...'.format(len(genomeToLines)), datetime.datetime.now()
                for genome in genomeToLines:
                    writeToGenome(genome, genomeToLines[genome])
                genomeToLines.clear()
                print 'collecting more lines...', datetime.datetime.now()

        # done with path.
        # write out collected lines to the various genome fasta files
        print 'finishing writing collected lines to fasta files for {} genomes...'.format(len(genomeToLines)), datetime.datetime.now()
        for genome in genomeToLines:
            writeToGenome(genome, genomeToLines[genome])
        genomeToLines.clear()
        print 'done writing collected lines.', datetime.datetime.now()

    print 'updating metadata...', datetime.datetime.now()
    setData(ds, GENE_TO_GO_TERMS, geneToGoTerms)
    setData(ds, GENE_TO_GENE_IDS, geneToGeneIds)
    setData(ds, GENE_TO_NAME, geneToName)
    setData(ds, GENE_TO_DESC, geneToDesc)
    setData(ds, GENE_TO_GENOME, geneToGenome)
    setData(ds, GENOME_TO_GENES, genomeToGenes)
    setGenomes(ds)
    updateMetadata(ds, {'genomeToName': genomeToName, 'genomeToTaxon': genomeToTaxon, 'genomeToCount': genomeToCount,
                        'genomeToOrgCode': genomeToOrgCode})

    print 'all done.', datetime.datetime.now()
    

def updateGenomeCounts(ds):
    '''
    Update dataset metadata with the sequence count of each genome fasta file.
    '''
    genomes = getGenomes(ds)
    genomeToCount = {}
    for g in genomes:
        genomeToCount[g] = fasta.numSeqsInFastaDb(getGenomeFastaPath(ds, g))
        print g, genomeToCount[g]
    updateMetadata(ds, {'genomeToCount': genomeToCount})


def formatGenomes(ds, onGrid=False, clean=False, jobSize=40):
    '''
    ds: dataset for which to format genomes
    onGrid: if true, formatting will broken into many jobs that get distributed on lsf grid. 
    clean: if True, all jobs will be run.  Otherwise only incomplete jobs will run.
    jobSize: granularity of splits when running onGrid.  Smaller = more jobs = done sooner, ideally.
    Format genomes for blast.  Formatting is broken into several jobs which are run serially
    on the local machine or distributed on the lsf grid.
    '''
    print 'formatting genomes. {}'.format(ds)
    genomes = getGenomes(ds) 
    random.shuffle(genomes) # shuffle so each job will have roughly the same amount of large and small genomes.

    # To speed formatting, break into jobs and parallelize on LSF.
    funcName = 'roundup_dataset.formatSomeGenomes'
    jobs = [(funcName, {'ds': ds, 'genomes': gs}) for gs in util.groupsOfN(genomes, jobSize)] # util.splitIntoN(genomes, 20)]
    print jobs
    ns = getDatasetId(ds) + '_format_genomes'
    lsfOptions = ['-q', config.LSF_SHORT_QUEUE]
    if clean:
        workflow.dropDones(ns)
    workflow.runJobs(ns, jobs, lsfOptions=lsfOptions, onGrid=onGrid, devnull=True)


def formatSomeGenomes(ds, genomes):
    for genome in genomes:
        fastaPath = getGenomeFastaPath(ds, genome)
        print 'format {}'.format(genome)
        rsd.formatForBlast(fastaPath)
        

def prepareJobs(ds, numJobs=2000, pairs=None):
    '''
    ds: dataset to ready to compute pairs
    numJobs: split pairs into this many jobs.  More jobs = shorter jobs, better parallelism.
      Fewer jobs = fewer dirs and files to make isilon run slowly and overload lsf queue.
      Recommendation: <= 10000.
    pairs: If None, compute orthologs for all pairs of genomes.  If not None, only compute these pairs.  useful for testing.  
    '''
    print 'prepare jobs for {}'.format(ds)

    if pairs is None:
        pairs = getPairs(ds)
    print 'pair count:', len(pairs)

    # create up to N jobs for the pairs to be computed.
    # each job contain len(pairs)/N pairs, except if N does not divide len(pairs) evenly, some jobs get an extra pair.
    # e.g. if you had 11 pairs (1,2,3,4,5,6,7,8,9,10,11) and 3 jobs, the pairs would be split
    # into these jobs: (1,2,3,4),(5,6,7,8),(9,10,11)
    # Distribute pairs randomly among jobs so that each job will have about the same running time.
    # Ideally job running time would be explictly balanced, but currently pairs are just assigned randomly.
    random.shuffle(pairs)
    for i, jobPairs in enumerate(util.splitIntoN(pairs, numJobs)):
        if i % 100 == 0:
            print 'preparing job', i
        job = 'job_{}'.format(i)
        print 'job:', job
        print 'len(jobPairs)=', len(jobPairs)
        jobDir = getJobDir(ds, job)
        print 'jobDir:', jobDir
        os.makedirs(getJobDir(ds, job), 0770)
        setJobPairs(ds, job, jobPairs)

    getJobs(ds, refresh=True) # refresh the cached metadata


def makeChangeLog(ds, others):
    '''
    others: a list of other datasets to compare this one too.
    '''
    dss = [ds] + others
    dsids = [getDatasetId(d) for d in dss]
    data = collections.defaultdict(dict)
    for d, dsid in zip(dss, dsids):
        print d
        genomes = getGenomes(d)
        print 'len(genomes)=', len(genomes)
        genomeToName = getGenomeToName(d)
        genomeToName = {g: genomeToName[g] for g in genomes}
        print 'len(genomeToName)=', len(genomeToName)
        genomeToTaxon = getGenomeToTaxon(d)
        genomeToTaxon = {g: genomeToTaxon[g] for g in genomes}
        print 'len(genomeToTaxon)=', len(genomeToTaxon)
        taxonToGenome = {genomeToTaxon[g]: g for g in genomeToTaxon}
        print 'len(taxonToGenome)=', len(taxonToGenome)
        taxonToName = {t: genomeToName[taxonToGenome[t]] for t in taxonToGenome}
        taxToData = getTaxonToData(d)
        taxToCat = {t: taxToData[t][CAT_CODE] for t in taxToData if CAT_CODE in taxToData[t] }
        taxons = [genomeToTaxon[g] for g in genomes]
        euks = [t for t in taxons if taxToCat.get(t) == 'E']
        assert len(taxons) == len(set(taxons)) == len(genomes) == len(set(genomes))
        taxons = set(taxons)
        euks = set(euks)
        def spec(name):
            return ' '.join(name.split()[:2])
        taxonNames = set(taxonToName[g] for g in taxons)
        eukNames = set([taxonToName[g] for g in euks])
        taxonSpecies = set([spec(n) for n in taxonNames])
        eukSpecies = set(spec(n) for n in eukNames)
        taxonSpeciesToNames = collections.defaultdict(set)
        [taxonSpeciesToNames[spec(g)].add(g) for g in taxonNames]
        multiTaxonSpecies = set([s for s in taxonSpeciesToNames if len(taxonSpeciesToNames[s]) > 1])
        eukSpeciesToNames = collections.defaultdict(set)
        [eukSpeciesToNames[spec(g)].add(g) for g in eukNames]
        multiEukSpecies = set([s for s in eukSpeciesToNames if len(eukSpeciesToNames[s]) > 1])
        data[dsid]['taxons'] = list(taxons) # convert to list b/c json does not serialize sets.  boo.
        data[dsid]['euks'] = list(euks)
        data[dsid]['taxonToName'] = taxonToName
        data[dsid]['taxonNames'] = list(taxonNames)
        data[dsid]['eukNames'] = list(eukNames)
        data[dsid]['taxonSpecies'] = list(taxonSpecies)
        data[dsid]['eukSpecies'] = list(eukSpecies)
        data[dsid]['multiTaxonSpecies'] = list(multiTaxonSpecies)
        data[dsid]['multiEukSpecies'] = list(multiEukSpecies)
        
    descs = 'taxons euks taxonSpecies eukSpecies multiTaxonSpecies multiEukSpecies'.split()
    # differences in taxonNames and eukNames are not interesting because names are not comparable when
    # we use ncbi taxonomy names in one release and uniprot organism names in another.
    diffs = collections.defaultdict(dict)
    for dsid1, dsid2 in itertools.permutations(dsids, 2):
        print 'difference', dsid1, ' - ', dsid2
        descDiffs = {}
        for desc in descs:
            diff = list(set(data[dsid1][desc]) - set(data[dsid2][desc]))
            size = len(diff)
            print desc, 'len({}) - len({}) = {}'.format(dsid1, dsid2, size)
            print '\n'.join(['\t'+str(x) for x in diff])
            descDiffs[desc] = {'difference': diff, 'size': size}
        diffs[dsid1][dsid2] = descDiffs
    sizes = {}
    for dsid in dsids:
        print 'sizes', dsid
        descSizes = {}
        for i, desc in enumerate(descs):
            size = len(data[dsid][desc])
            print desc, 'len({}) = {}'.format(dsid, size)
            descSizes[desc] = size
        sizes[dsid] = descSizes
    for dsid1, dsid2 in itertools.permutations(dsids, 2):
        print 'difference', dsid1, ' - ', dsid2
        diff = list(set(data[dsid1]['euks']) - set(data[dsid2]['euks']))
        print '\n'.join([str(t) + '\t' + str(data[dsid1]['taxonToName'][t]) for t in diff])
    changeLog = {'data': data, 'differences': diffs, 'sizes': sizes}
    setData(ds, CHANGE_LOG, changeLog)
                


#################################
# DOWNLOAD AND ORTHOXML FUNCTIONS
#################################

def processGenomesForDownload(ds):
    '''
    copy genome fasta files into a dir under download dir, then tar.gz the dir.
    '''
    downloadDir = getDownloadDir(ds)
    genomes = getGenomes(ds)
    downloadGenomesDir = os.path.join(downloadDir, 'genomes')
    if not os.path.exists(downloadGenomesDir):
        os.mkdir(downloadGenomesDir, DIR_MODE)
    for genome in genomes:
        print genome
        shutil.copy(getGenomeFastaPath(ds, genome), downloadGenomesDir)

    print 'tarring dir'
    subprocess.check_call(['tar', '-czf', 'genomes.tar.gz', 'genomes'], cwd=downloadDir)
    print 'deleting dir'
    shutil.rmtree(downloadGenomesDir)


def getDownloadGenomesPath(ds):
    return os.path.join(getDownloadDir(ds), 'genomes.tar.gz')


def getDownloadOrthologsPath(ds, div, evalue):
    downloadDir = getDownloadDir(ds)
    return os.path.join(downloadDir, '{}_{}.orthologs.txt.gz'.format(div, evalue))

    
def collateOrthologs(ds):
    '''
    collate orthologs from jobs files into files for each parameter combination in the download dir.
    '''

    divEvalues, txtPaths, xmlPaths = getDownloadPaths(ds)
    # open files
    divEvalueToFH = dict([(divEvalue, open(path, 'w')) for divEvalue, path in zip(divEvalues, txtPaths)])
    try:
        print 'collating orthologs'
        for orthData in orthutil.orthDatasFromFilesGen(getOrthologsFiles(ds)):
            params, orthologs = orthData
            qdb, sdb, div, evalue = params
            orthutil.orthDatasToStream([orthData], divEvalueToFH[(div, evalue)])
    finally:
        for fh in divEvalueToFH.values():
            fh.close()


def zipDownloadPaths(ds, onGrid=False, clean=False):
    '''
    after orthologs have been collated and converted to orthoxml, zip the files
    '''
    divEvalues, txtPaths, xmlPaths = getDownloadPaths(ds)
    ns = getDatasetId(ds) + '_zip_download'
    jobs = [('roundup_dataset.zipDownloadPath', {'path': path}) for path in txtPaths + xmlPaths if os.path.exists(path)]
    lsfOptions = ['-q '+config.LSF_LONG_QUEUE]
    if clean:
        workflow.dropDones(ns)
    workflow.runJobs(ns, jobs, lsfOptions=lsfOptions, onGrid=onGrid, devnull=True)


def zipDownloadPath(path):
        print 'gzipping', path
        subprocess.check_call(['gzip', path])


def getDownloadPaths(ds):
    downloadDir = getDownloadDir(ds)
    divEvalues = list(roundup_common.genDivEvalueParams())
    txtPaths = [os.path.join(downloadDir, '{}_{}.orthologs.txt'.format(div, evalue)) for div, evalue in divEvalues]
    xmlPaths = [re.sub('\.txt$', '.xml', path) for path in txtPaths]
    return divEvalues, txtPaths, xmlPaths


def convertToOrthoXML(ds, origin, originVersion, databaseName, databaseVersion, protLink=None, onGrid=True, clean=False):
    '''
    origin: e.g. 'roundup'
    originVersion: e.g. getReleaseName(ds)
    databaseName: e.g. 'Uniprot'
    databaseVersion: e.g. getUniprotRelease(ds)
    protLink: e.g. 'http://www.uniprot.org/uniprot/'
    convert collated orthologs files to orthoxml format.
    clean: if True, will delete the xml files if they already exist, and also clean the dones for these jobs
    onGrid: if True, will convert txt files to xml in parallel on lsf.  Otherwise, will run serially in this process.
    '''
    divEvalues, txtPaths, xmlPaths = getDownloadPaths(ds)
    ns = getDatasetId(ds) + '_to_orthoxml'
    jobs = []
    for txtPath, xmlPath in zip(txtPaths, xmlPaths):
        func = 'roundup_dataset.convertTxtToOrthoXML'
        kws = {'ds': ds, 'txtPath': txtPath, 'xmlPath': xmlPath,
               'origin': origin, 'originVersion': originVersion, 
               'databaseName': databaseName, 'databaseVersion': databaseVersion, 'protLink': protLink}
        jobs.append((func, kws))
    lsfOptions = ['-q '+config.LSF_LONG_QUEUE]
    if clean:
        for xmlPath in xmlPaths:
            if os.path.exists(xmlPath):
                os.remove(xmlPath)
        workflow.dropDones(ns)
    workflow.runJobs(ns, jobs, lsfOptions=lsfOptions, onGrid=onGrid, devnull=True)


def convertTxtToOrthoXML(ds, txtPath, xmlPath, origin, originVersion, databaseName, databaseVersion, protLink):
    '''
    WARNING: xml files on the production roundup dataset are too big.  This step should be skipped.
    txtPath: a orthdatas file
    xmlPath: where to write the OrthoXML version of txtPath
    '''
    print txtPath, '=>', xmlPath
    with open(xmlPath, 'w') as xmlOut:
        convertOrthDatasToXml(ds, orthutil.orthDatasFromFileGen(txtPath), orthutil.orthDatasFromFileGen(txtPath), xmlOut,
                              origin, originVersion, databaseName, databaseVersion, protLink)


def makeOrthoxmlSpecies(genomeToGenes, genomeToName, genomeToTaxon, databaseName, databaseVersion, protLink):
    '''
    Create a orthoxml.Species object for each genome in genomeToGenes, generating document-unique ids for each gene.
    returns: a list of orthoxml.Species objects, and a dict from gene to the integer id used as a gene id
    for the genes in the Species list.
    '''
    speciesList = []
    geneToIden = {}
    iden = 0 # orthoxml wants a interger gene id for each gene, internal to the xml document.
    for genome in genomeToGenes:
        genes = []
        for gene in genomeToGenes[genome]:
            iden += 1
            genes.append(orthoxml.Gene(iden, protId=gene))
            geneToIden[gene] = iden
        # genes = [orthoxml.Gene(gene, protId=gene) for gene in genomeToGenes[genome]]
        database = orthoxml.Database(databaseName, databaseVersion, genes=genes, protLink=protLink)
        species = orthoxml.Species(genomeToName[genome], genomeToTaxon[genome], [database])
        speciesList.append(species)
    return speciesList, geneToIden


def convertOrthDatasToXml(ds, orthDatas, orthDatasAgain, xmlOut,
        origin='roundup', originVersion=None, databaseName='Uniprot',
        databaseVersion=None, protLink="http://www.uniprot.org/uniprot/"):
    '''
    orthDatas: iter of orthDatas
    orthDatasAgain: another iter of the same orthDatas.  
    xmlOut: a stream in which to write serialized orthoxml.
    a 2-pass method for converting orthdatas for a dataset to orthoxml
    pass through orthologs, collecting genome to gene.
    pass through again, writing out species, scores, groups.
    add parameters to orthoxml notes.
    Done this way to avoid needing to have all orthDatas in memory, in case there are millions of them.
    '''
    if originVersion is None:
        originVersion = getReleaseName(ds)
    
    if databaseVersion is None:
        databaseVersion = getUniprotRelease(ds)

    print 'getting metadata'
    genomeToName = getGenomeToName(ds)
    genomeToTaxon = getGenomeToTaxon(ds)
    
    print 'pass 1: collecting genes'
    genomeToGenes = collections.defaultdict(set)
    for params, orthologs in orthDatas:
        qdb, sdb, div, evalue = params
        for qid, sid, dist in orthologs:
            genomeToGenes[qdb].add(qid)
            genomeToGenes[sdb].add(sid)
        
    print 'making species'
    speciesList, geneToIden = makeOrthoxmlSpecies(genomeToGenes, genomeToName,
            genomeToTaxon, databaseName, databaseVersion, protLink)

    scoreDef = orthoxml.ScoreDef('dist', 'Maximum likelihood evolutionary distance')

    print 'pass 2: writing xml'

    def groupGen():
        for params, orthologs in orthDatasAgain:
            qdb, sdb, div, evalue = params
            for qid, sid, dist in orthologs:
                qGeneRef = orthoxml.GeneRef(geneToIden[qid])
                sGeneRef = orthoxml.GeneRef(geneToIden[sid])
                group = orthoxml.OrthologGroup([qGeneRef, sGeneRef], scores=[orthoxml.Score(scoreDef.id, dist)])
                yield group

    notes = orthoxml.Notes('These orthologs were computed using the following Reciprocal Smallest Distance (RSD) parameters: divergence={} and evalue={}.  See http://roundup.hms.harvard.edu for more information about Roundup and RSD.'.format(div, evalue))
    for xmlText in orthoxml.toOrthoXML(origin, originVersion, speciesList, groupGen(), scoreDefs=[scoreDef], notes=notes):
        xmlOut.write(xmlText)


def convertOrthGroupsToXml(ds, groups, genomeToGenes, div, evalue, xmlOut,
        origin='roundup', originVersion=None, databaseName='Uniprot',
        databaseVersion=None, protLink="http://www.uniprot.org/uniprot/"):
    '''
    ds: used to lookup genome names, taxons, roundup dataset release, and uniprot release.
    groups: a iterable of tuples of (genes, avgDist) for a group of orthologous genes.  avgDist is the mean distance of all orthologous pairs in the group.
    genomeToGenes: a dict from genome to the genes in that genome.
    div: the divergence threshold used to compute the orthologs in groups
    evalue: the evalue threshold used to compute the orthologs in groups
    xmlOut: a stream (e.g. filehandle) that the orthoxml is written to.
    '''
    if originVersion is None:
        originVersion = getReleaseName(ds)
    
    if databaseVersion is None:
        databaseVersion = getUniprotRelease(ds)

    print 'getting metadata'
    genomeToName = getGenomeToName(ds)
    genomeToTaxon = getGenomeToTaxon(ds)
    
    print 'making species'
    speciesList, geneToIden = makeOrthoxmlSpecies(genomeToGenes, genomeToName,
            genomeToTaxon, databaseName, databaseVersion, protLink)

    scoreDef = orthoxml.ScoreDef('avgdist', 'Mean maximum likelihood evolutionary distance of all orthologous pairs in a group')
    
    print 'writing xml'

    def groupGen():
        for genes, avgDist in groups:
            geneRefs = [orthoxml.GeneRef(geneToIden[gene]) for gene in genes]
            score = orthoxml.Score(scoreDef.id, str(avgDist))
            group = orthoxml.OrthologGroup(geneRefs, scores=[score])
            yield group

    notes = orthoxml.Notes('These orthologs were computed using the following Reciprocal Smallest Distance (RSD) parameters: divergence={} and evalue={}.  See http://roundup.hms.harvard.edu for more information about Roundup and RSD.'.format(div, evalue))
    for xmlText in orthoxml.toOrthoXML(origin, originVersion, speciesList, groupGen(), scoreDefs=[scoreDef], notes=notes):
        xmlOut.write(xmlText)


###############
# DATASET STUFF
###############

def getDatasetId(ds):
    '''
    e.g. 3
    '''
    return os.path.basename(ds)


def getGenomesDir(ds):
    return os.path.join(ds, 'genomes')


def getJobsDir(ds):
    return os.path.join(ds, 'jobs')

    
def getOrthologsDir(ds):
    return os.path.join(ds, 'orthologs')

    
def getSourcesDir(ds):
    return os.path.join(ds, 'sources')


def getDownloadDir(ds):
    return os.path.join(ds, 'download')


def getDats(ds):
    sourcesDir = getSourcesDir(ds)
    return [os.path.join(sourcesDir, 'uniprot', f) for f in ['uniprot_sprot.dat', 'uniprot_trembl.dat']]


###################
# CLEANING FUNCTION
###################



def cleanOrthologs(ds):
    '''
    remove everything in the orthologs dir.  useful for resetting a computation
    '''
    print 'removing orthologs'
    od = getOrthologsDir(ds)
    for path in [os.path.join(od, f) for f in os.listdir(od)]:
        print 'removing {}'.format(path)
        os.remove(path)


def cleanJobs(ds):
    '''
    removes all the jobs in the dataset.  can be useful for clearing out a test computation
    '''
    # remove all job dirs
    print 'removing jobs'
    jobs = getJobs(ds, refresh=True)
    for job in jobs:
        path = getJobDir(ds, job)
        if os.path.exists(path):
            print 'removing {}'.format(path)
            shutil.rmtree(path)
    # reset jobs cache
    getJobs(ds, refresh=True)
    # drop the table that stores job completes
    print 'resetting completes...'
    resetCompletes(ds)
    print 'resetting stats...'
    resetStats(ds)


def cleanGenomes(ds):
    '''
    remove everything in the genomes dir and reset the genomes list.
    '''
    # delete everything
    print 'deleting everything in genomes dir'
    genomesDir = getGenomesDir(ds)
    everything = [os.path.join(genomesDir, x) for x in os.listdir(genomesDir)]
    for path in everything:
        print 'removing {}'.format(path)
        shutil.rmtree(path)
        
    # reset genomes cache
    print 'resetting genomes cache'
    setGenomes(ds)

    # reset dones for formatting.
    print 'clearing format genomes dones'
    ns = getDatasetId(ds) + '_format_genomes'
    workflow.dropDones(ns)


##########
# RUN JOBS
##########

def computeJobs(ds):
    '''
    submit all incomplete and non-running jobs to lsf, so they can compute their respective pairs.
    returns: True if all jobs are done.
    '''
    # awkward: a dataset job is a name of a directory and a set of pairs to compute orthologs for.
    # and a workflow job is a tuple of function name and keywords
    jobs = sorted(getJobs(ds))
    func = 'roundup_dataset.computeJob'
    ns = getDatasetId(ds) + '_compute_jobs'
    workflowJobs = [(func, {'ds': ds, 'job': job}) for job in jobs]
    lsfOptions = ['-R', 'rusage[tmp=500]', '-q', config.LSF_LONG_QUEUE]
    return workflow.runJobs(ns, workflowJobs, names=jobs, lsfOptions=lsfOptions, onGrid=True, devnull=True)


def computeJob(ds, job):
    '''
    job: identifies which job this is so it knows which pairs to compute.
    computes orthologs for every pair in the job.  merges the orthologs into a single file and puts that file in the dataset orthologs dir.
    '''
    pairs = getJobPairs(ds, job)
    jobDir = getJobDir(ds, job)
    print ds, job, pairs, jobDir

    # compute orthologs for pairs
    for pair in pairs:
        if not isComplete(ds, 'pair', pair):
            orthologsPath = os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair))
            print orthologsPath
            computePair(ds, pair, jobDir, orthologsPath)
            markComplete(ds, 'pair', pair)

    # merge orthologs for pairs into a single file.
    if not isComplete(ds, 'job_ologs_merge', job):
        jobOrthologsPath = getJobOrthologsPath(ds, job)
        pairsPaths = [os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair)) for pair in pairs]
        orthDatasGen = orthutil.orthDatasFromFilesGen(pairsPaths)
        orthutil.orthDatasToFile(orthDatasGen, jobOrthologsPath)
        markComplete(ds, 'job_ologs_merge', job)
        
    # delete the individual pair olog files
    for pair in pairs:
        orthologsPath = os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair))
        if os.path.exists(orthologsPath):
            os.remove(orthologsPath)

            
def computePair(ds, pair, workingDir, orthologsPath):
    '''
    ds: the roundup dataset.
    pair: find orthologs for this pair of genomes.
    workingDir: where to save blast hits and orthologs as they get completed.
    orthologsPath: where to write the orthologs.
    precompute blast hits for pair, then compute orthologs for pair, then write orthologs to a file and clean up other files.
    '''
    # a pair is complete when it has written its orthologs to a file and cleaned up its other data files.
    pairStartTime = time.time()
    queryGenome, subjectGenome = pair
    queryGenomePath = getGenomePath(ds, queryGenome)
    subjectGenomePath = getGenomePath(ds, subjectGenome)
    queryFastaPath = getGenomeFastaPath(ds, queryGenome)
    subjectFastaPath = getGenomeFastaPath(ds, subjectGenome)
    queryIndexPath = getGenomeIndexPath(ds, queryGenome)
    subjectIndexPath = getGenomeIndexPath(ds, subjectGenome)
    forwardHitsPath = os.path.join(workingDir, '{}_{}.forward_hits.pickle'.format(*pair))
    reverseHitsPath = os.path.join(workingDir, '{}_{}.reverse_hits.pickle'.format(*pair))
    maxEvalue = max([float(evalue) for evalue in roundup_common.EVALUES]) # evalues are strings like '1e-5'
    divEvalues = list(roundup_common.genDivEvalueParams())    
    with nested.NestedTempDir(dir=roundup_common.LOCAL_DIR) as tmpDir:
        logging.debug('computePair. tmpDir={}'.format(tmpDir))
        if not isComplete(ds, 'blast', queryGenome, subjectGenome):
            startTime = time.time()
            rsd.computeBlastHits(queryFastaPath, subjectIndexPath, forwardHitsPath, maxEvalue,
                                 workingDir=tmpDir, copyToWorking=roundup_common.ROUNDUP_LOCAL)
            putBlastStats(ds, queryGenome, subjectGenome, startTime=startTime, endTime=time.time())
            markComplete(ds, 'blast', queryGenome, subjectGenome)

        if not isComplete(ds, 'blast', subjectGenome, queryGenome):
            startTime = time.time()
            rsd.computeBlastHits(subjectFastaPath, queryIndexPath, reverseHitsPath, maxEvalue,
                                 workingDir=tmpDir, copyToWorking=roundup_common.ROUNDUP_LOCAL)
            putBlastStats(ds, subjectGenome, queryGenome, startTime=startTime, endTime=time.time())
            markComplete(ds, 'blast', subjectGenome, queryGenome)

        if not isComplete(ds, 'roundup', pair):
            startTime = time.time()
            divEvalueToOrthologs = rsd.computeOrthologsUsingSavedHits(queryFastaPath, subjectFastaPath, divEvalues,
                                                                      forwardHitsPath, reverseHitsPath, workingDir=tmpDir)
            orthDatas = [((queryGenome, subjectGenome, div, evalue), orthologs) for (div, evalue), orthologs in divEvalueToOrthologs.items()]
            orthutil.orthDatasToFile(orthDatas, orthologsPath)
            putRsdStats(ds, queryGenome, subjectGenome, divEvalues, startTime=startTime, endTime=time.time())
            markComplete(ds, 'roundup', pair)
    # clean up files
    if os.path.exists(forwardHitsPath):
        os.remove(forwardHitsPath)
    if os.path.exists(reverseHitsPath):
        os.remove(reverseHitsPath)
    # complete pair computation
    putPairStats(ds, queryGenome, subjectGenome, startTime=pairStartTime, endTime=time.time())


######
# JOBS
######

def getJobs(ds, refresh=False):
    '''
    caches jobs in the dataset metadata if they have not already been
    cached, b/c the isilon is wicked slow at listing dirs.
    returns: list of jobs in the dataset.
    '''
    if refresh:
        return updateMetadata(ds, {'jobs': os.listdir(getJobsDir(ds))})['jobs']
    else:
        jobs = getMetadata(ds).get('jobs')
        if not jobs:
            return updateMetadata(ds, {'jobs': os.listdir(getJobsDir(ds))})['jobs']
        else:
            return jobs


def getJobPairs(ds, job):
    with open(os.path.join(getJobDir(ds, job), 'job_pairs.json')) as fh:
        return json.load(fh)


def setJobPairs(ds, job, pairs):
    with open(os.path.join(getJobDir(ds, job), 'job_pairs.json'), 'w') as fh:
        json.dump(pairs, fh, indent=0)
    


def getJobDir(ds, job):
    return os.path.join(getJobsDir(ds), job)


def getJobOrthologsPath(ds, job):
    return os.path.join(getOrthologsDir(ds), '{}.orthologs.txt'.format(job))


def getComputeJobName(ds, job):
    return getDatasetId(ds) + '_' + job


#########
# GENOMES
#########

def getGenomes(ds):
    '''
    gets genomes cached in the metadata.
    returns: list of genomes in the dataset.
    '''
    return getMetadata(ds)['genomes']


def setGenomes(ds, genomes=None):
    '''
    genomes: a list of genomes to cache.  If None, will get the list from the genomes directory.
    caches genomes in the dataset metadata, b/c the isilon is wicked slow at listing dirs.
    also useful for testing.  can make a small dataset.
    '''
    if genomes is None:
        genomes = os.listdir(getGenomesDir(ds))
    return updateMetadata(ds, {'genomes': genomes})['genomes']

    
def getGenomesAndPaths(ds):
    '''
    returns: a dict mapping every genome in the dataset to its genomePath.
    '''
    genomesAndPaths = {}
    genomesDir = getGenomesDir(ds)
    for genome in getGenomes(ds):
        genomesAndPaths[genome] = os.path.join(genomesDir, genome)
    return genomesAndPaths


def getGenomePath(ds, genome):
    '''
    a genomePath is a directory containing genome fasta files and blast indexes.
    '''
    return os.path.join(getGenomesDir(ds), genome)


def getGenomeFastaPath(ds, genome):
    '''
    location of fasta file for a genome
    '''
    # .faa for fasta files containing amino acids.  http://en.wikipedia.org/wiki/FASTA_format#File_extension
    return os.path.join(getGenomePath(ds, genome), genome+'.faa')


def getGenomeIndexPath(ds, genome):
    '''
    location of blast index files.
    '''
    return os.path.join(getGenomePath(ds, genome), genome+'.faa')

    

###################
# ORTHOLOGS/RESULTS
###################

    
def getOrthologsFiles(ds):
    return glob.glob(os.path.join(getOrthologsDir(ds), '*.orthologs.txt'))


#######
# PAIRS
#######

def getPairs(ds, genomes=None):
    '''
    returns: a sorted list of pairs, where every pair is a sorted list of each combination of two genomes.
    '''
    if genomes is None:
        genomes = getGenomes(ds)
    return sorted(set([tuple(sorted((g1, g2))) for g1 in genomes for g2 in genomes if g1 != g2]))


###########
# COMPLETES
###########
# Used to mark various steps in creating a dataset complete so they do not need to be redone.
# isComplete uses the mysql database.  Used for concurrent execution of jobs, pairs, and other large numbers of completes.
# pros: concurrency. fast even with millions of completes.
# cons: different mysql db for dev and prod, so must use the prod code on a prod dataset,
#   or point the dev code to the prod database


def isComplete(ds, *key):
    return workflow.isDone(completesNS(ds), key)


def markComplete(ds, *key):
    workflow.markDone(completesNS(ds), key)


def unmarkComplete(ds, *key):
    workflow.unmarkDone(completesNS(ds), key)


def resetCompletes(ds):
    return workflow.resetDones(completesNS(ds))


def completesNS(ds):
    return 'roundup_dataset_{}'.format(getDatasetId(ds))


##########
# METADATA
##########
# The metadata of a dataset is persistent information describing the dataset.
# It is stored in a file, so is not safe for concurrent writing.

def updateMetadata(ds, metadata):
    '''
    metadata: a dict containing information about the dataset.  e.g. source files, download times, genome names.
    update existing dataset metadata with the values in metadata.
    returns: the updated dataset metadata.
    '''
    md = getMetadata(ds)
    md.update(metadata)
    return setMetadata(ds, md)

    
def getMetadata(ds):
    '''
    returns: a dict, the existing persisted dataset metadata.
    '''
    return getData(ds, DATASET)
    

def setMetadata(ds, metadata):
    return setData(ds, DATASET, metadata)


def getData(ds, key):
    '''
    used for storing big chunks of metadata, like geneToDesc.
    '''
    path = os.path.join(ds, 'metadata.{}.json'.format(key))
    with open(path) as fh:
        return json.load(fh)


def setData(ds, key, data):
    '''
    used for storing big chunks of metadata, like geneToDesc.
    '''
    path = os.path.join(ds, 'metadata.{}.json'.format(key))
    with open(path, 'w') as fh:
        json.dump(data, fh, indent=0)
    return data


def getUniprotRelease(ds):
    return getMetadata(ds)['uniprotRelease']


def getSourceUrls(ds):
    return getMetadata(ds)['sources']

        
def getReleaseName(ds):
    '''
    returns: the name of this roundup release, in human readable form.  e.g. 2
    '''
    return getDatasetId(ds)


def getReleaseDate(ds):
    '''
    returns: a datetime.date object
    '''
    dateStr = getMetadata(ds)['releaseDate']
    return datetime.datetime.strptime(dateStr, "%Y-%m-%d").date()

    
def setReleaseDate(ds, date=None):
    '''
    date: a datetime.date object.  if None, datetime.date.today() is used.
    '''
    if date is None:
        date = datetime.date.today()
    updateMetadata(ds, {'releaseDate': str(date.strftime("%Y-%m-%d"))})
    

def getGenomeToName(ds):
    return getMetadata(ds)['genomeToName']


def getGenomeToTaxon(ds):
    return getMetadata(ds)['genomeToTaxon']


def getGenomeToCount(ds):
    return getMetadata(ds)['genomeToCount']


def getDatasetStats(ds):
    md = getMetadata(ds)
    return dict((key, md[key]) for key in ('numGenomes', 'numPairs', 'numOrthologs'))


def getGenomeToGenes(ds):
    return getData(ds, GENOME_TO_GENES)


def getGeneToName(ds):
    return getData(ds, GENE_TO_NAME)


def getGeneToGenome(ds):
    return getData(ds, GENE_TO_GENOME)


def getGeneToGoTerms(ds):
    return getData(ds, GENE_TO_GO_TERMS)


def getGeneToGeneIds(ds):
    return getData(ds, GENE_TO_GENE_IDS)


def getTermToData(ds):
    return getData(ds, TERM_TO_DATA)


def getTaxonToData(ds):
    return getData(ds, TAXON_TO_DATA)


#################
# STATS FUNCTIONS
#################


STATS_CACHE = {}


def getStatsStore(ds):
    if ds not in STATS_CACHE:
        dsId = getDatasetId(ds)
        # kv = kvstore.KVStore(util.ClosingFactoryCM(config.openDbConn), ns='roundup_dataset_{}_{}'.format(dsId, 'stats'))
        kv = kvstore.KVStore(util.FactoryCM(dbutil.Reuser(config.openDbConn)), ns='roundup_dataset_{}_{}'.format(dsId, 'stats'))
        STATS_CACHE[ds] = kv
    return STATS_CACHE[ds]
    

def resetStats(ds):
    getStatsStore(ds).reset()


def deleteStats(ds):
    getStatsStore(ds).delete()


def getStats(ds, key):
    '''
    returns: a dict of stats.  if key is not present, returns an empty dict
    '''
    return getStatsStore(ds).get(key, default={})


def putStats(ds, key, stats):
    '''
    stats: a dict of statistics for blast, rsd, or a pair.
    '''
    getStatsStore(ds).put(key, stats)


def putBlastStats(ds, qdb, sdb, startTime, endTime):
    stats = {'type': 'blast', 'qdb': qdb, 'sdb': sdb, 'startTime': startTime, 'endTime': endTime}
    key = ('blast', qdb, sdb)
    putStats(ds, key, stats)
    return stats


def getBlastStats(ds, qdb, sdb):
    key = ('blast', qdb, sdb)
    return getStats(ds, key)

    
def putRsdStats(ds, qdb, sdb, divEvalues, startTime, endTime):
    stats = {'type': 'roundup', 'qdb': qdb, 'sdb': sdb, 'divEvalues': divEvalues, 'startTime': startTime, 'endTime': endTime}
    key = ('rsd', qdb, sdb)
    putStats(ds, key, stats)
    return stats


def getRsdStats(ds, qdb, sdb):
    key = ('rsd', qdb, sdb)
    return getStats(ds, key)

    
def putPairStats(ds, qdb, sdb, startTime, endTime):
    stats = {'type': 'pair', 'qdb': qdb, 'sdb': sdb, 'startTime': startTime, 'endTime': endTime}
    key = ('pair', qdb, sdb)
    putStats(ds, key, stats)
    return stats


def getPairStats(ds, qdb, sdb):
    key = ('pair', qdb, sdb)
    return getStats(ds, key)


def extractDatasetStats(ds):
    '''
    update the dataset metadata with num genomes, num pairs, total num orthologs.
    '''
    numGenomes = len(getGenomes(ds))
    numPairs = len(getPairs(ds))
    numOrthologs = 0
    for params, orthologs in orthutil.orthDatasFromFilesGen(getOrthologsFiles(ds)):
        numOrthologs += len(orthologs)
    updateMetadata(ds, {'numGenomes': numGenomes, 'numPairs': numPairs, 'numOrthologs': numOrthologs})

    
def extractPerformanceStats(ds, pairs=None):
    '''
    pairs: default is to collect stats for all pairs.  It can be useful for testing to set pairs to a specific list of pairs.
    times are in seconds.
    '''
    blastStats = {}
    rsdStats = {}
    totalTime = 0
    totalRsdTime = 0
    totalBlastTime = 0
    
    if pairs is None:
        pairs = getPairs(ds)

    def elapsedTime(stats):
        return stats['endTime'] - stats['startTime']
    
    for qdb, sdb in pairs:
        forwardTime = elapsedTime(getBlastStats(ds, qdb, sdb))
        reverseTime = elapsedTime(getBlastStats(ds, sdb, qdb))
        rsdTime = elapsedTime(getRsdStats(ds, qdb, sdb))

        blastStats[json.dumps((qdb, sdb))] = forwardTime
        blastStats[json.dumps((sdb, qdb))] = reverseTime
        rsdStats[json.dumps((qdb, sdb))] = rsdTime

        totalTime += forwardTime + reverseTime + rsdTime
        totalBlastTime += forwardTime + reverseTime
        totalRsdTime += rsdTime

    print 'total time:', totalTime
    print 'total blast time:', totalBlastTime
    print 'total rsd time:', totalRsdTime
    print 'saving stats'
    setData(ds, BLAST_STATS, blastStats)
    setData(ds, RSD_STATS, rsdStats)
       

def main():
    '''
    Command line functionality.  These commands are meant to simplify the
    syntax for making a dataset.
    '''

    # wrappers to transform arguments from subparsers into the
    # appropriate function call.
    def prepare_dataset(args):
        return prepareDataset(args.dataset)

    def set_genomes(args):
        return setGenomes(args.dataset)

    def format(args):
        return formatGenomes(args.dataset, clean=True)

    def prepare_jobs(args):
        return prepareJobs(args.dataset, numJobs=args.num)

    def compute(args):
        return computeJobs(args.dataset)

    def collate(args):
        return collateOrthologs(args.dataset)

    def convert_to_orthoxml(args):
        return convertToOrthoXML(args.dataset, args.origin, 
                args.origin_version, args.database, args.database_version, 
                onGrid=True, clean=True)

    
    parser = argparse.ArgumentParser(description='')
    subparsers = parser.add_subparsers(dest='action')

    # prepare_dataset 
    prepare_dataset_parser = subparsers.add_parser('prepare_dataset', help='initialize the dataset directory and tables.')
    prepare_dataset_parser.add_argument('dataset', help='root directory of the dataset')
    prepare_dataset_parser.set_defaults(func=prepare_dataset)

    # set_genomes metadata
    set_genomes_parser = subparsers.add_parser('set_genomes', help='set genomes metadata in the dataset')
    set_genomes_parser.add_argument('dataset', help='root directory of the dataset')
    set_genomes_parser.set_defaults(func=set_genomes)

    # format genomes
    format_parser = subparsers.add_parser('format', help='format genomes in the dataset')
    format_parser.add_argument('dataset', help='root directory of the dataset')
    format_parser.set_defaults(func=format)

    # prepare ortholog jobs 
    prepare_jobs_parser = subparsers.add_parser('prepare_jobs', help='prepare_jobs genomes in the dataset')
    prepare_jobs_parser.add_argument('dataset', help='root directory of the dataset')
    prepare_jobs_parser.add_argument('-n', '--num', type=int, required=True,
            help='number of jobs in which to split the dataset pairs. e.g. 1000')
    prepare_jobs_parser.set_defaults(func=prepare_jobs)

    # compute orthologs
    compute_parser = subparsers.add_parser('compute', help='compute orthologs')
    compute_parser.add_argument('dataset', help='root directory of the dataset')
    compute_parser.set_defaults(func=compute) 
    
    # collate orthologs
    collate_parser = subparsers.add_parser('collate', help='collate orthologs in the dataset')
    collate_parser.add_argument('dataset', help='root directory of the dataset')
    collate_parser.set_defaults(func=collate)

    # convert_to_orthoxml
    convert_to_orthoxml_parser = subparsers.add_parser('convert_to_orthoxml', 
            help='convert dataset orthologs to orthoxml')
    convert_to_orthoxml_parser.add_argument('dataset', help='root directory of the dataset')
    convert_to_orthoxml_parser.add_argument('--origin', required=True, help='The source of the orthologs. e.g. roundup.')
    convert_to_orthoxml_parser.add_argument('--origin-version', required=True, help='The version of the orthologs. e.g. 3.0')
    convert_to_orthoxml_parser.add_argument('--database', required=True, help='The database name of the source of genomes. e.g. Uniprot')
    convert_to_orthoxml_parser.add_argument('--database-version', required=True, help='The version of the source genomes. e.g. 2012_04')
    convert_to_orthoxml_parser.set_defaults(func=convert_to_orthoxml)

    # parse command line arguments and invoke the appropriate handler.
    args = parser.parse_args()
    args.func(args)


if __name__ == '__main__':
    main()

########################
# DEPRECATED / UNUSED
########################



# last line - python emacs bug fix
 
